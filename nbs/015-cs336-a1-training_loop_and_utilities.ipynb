{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bca9b",
   "metadata": {},
   "source": [
    "# CS336 Assignments\n",
    "\n",
    "| # | Topic                         | Description                                 |\n",
    "|---|-------------------------------|---------------------------------------------|\n",
    "| 1 | Basics                        | Train an LLM from scratch                   |\n",
    "| 2 | Systems                       | Make it run fast!                           |\n",
    "| 3 | Scaling                       | Make it performant at a FLOP budget         |\n",
    "| 4 | Data                          | Prepare the right datasets                  |\n",
    "| 5 | Alignment & Reasoning RL      | Align it to real-world use cases            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f19a5c",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "- Implement all of the components (tokenizer, model, loss function, optimizer) necessary to train a standard Transformer language model\n",
    "- Train a minimal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fad44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c035e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropyloss(preds, actuals):\n",
    "    probs = preds.squeeze().sigmoid()\n",
    "    eps = 1e-8\n",
    "    loss = - actuals * (probs + eps).log() - (1 - actuals) * ((1 - probs) + eps).log()\n",
    "    return loss.mean()\n",
    "\n",
    "class AdamW_custom():\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2):\n",
    "        # defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # super().__init__(params, defaults)\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.wd = weight_decay\n",
    "        self.eps = eps\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Initialize momentum and velocity for each parameter\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad = param.grad.data\n",
    "\n",
    "            # Apply weight decay directly to parameters (AdamW style)\n",
    "            param.data.mul_(1 - self.lr * self.wd)\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            # beta1 * i + (1-beta1) * grad\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad.pow(2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.step_count)\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.step_count)\n",
    "\n",
    "            # Update parameters\n",
    "            param.data.add_(m_hat / (v_hat.sqrt() + self.eps), alpha = -self.lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b879e",
   "metadata": {},
   "source": [
    "## layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e74ad8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[0., 1.286] μ=0.331 σ=0.392 grad ReluBackward0\n",
       "tensor([[0.2151, 0.1047, 0.0000, 0.0000, 0.2167, 0.0521],\n",
       "        [0.4768, 0.0000, 0.7975, 0.3168, 1.2863, 0.6434],\n",
       "        [0.0000, 0.0000, 0.9230, 0.7498, 0.1746, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFN()\n",
    "output = model(data)\n",
    "\n",
    "output.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9b941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[3] x∈[0.098, 0.587] μ=0.331 σ=0.245 grad MeanBackward1 [0.098, 0.587, 0.308],\n",
       " tensor[3] x∈[0.010, 0.193] μ=0.126 σ=0.101 grad VarBackward0 [0.010, 0.193, 0.175])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = output.mean(dim=-1)\n",
    "var = output.var(dim=-1)\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292aa088",
   "metadata": {},
   "source": [
    "We see that the mean of each layer is not zero and the standard deviation is not close to 1. With layer normalization, we can bring them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09da29ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m (\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m) / torch.sqrt(var)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f7503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[3, 1] x∈[0.098, 0.587] μ=0.331 σ=0.245 grad MeanBackward1 [[0.098], [0.587], [0.308]],\n",
       " tensor[3, 1] x∈[0.010, 0.193] μ=0.126 σ=0.101 grad VarBackward0 [[0.010], [0.193], [0.175]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "759eb4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[-1.335, 1.591] μ=-6.623e-09 σ=0.939 grad DivBackward0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87eb5c",
   "metadata": {},
   "source": [
    "0 mean and 1 variance! :) \n",
    "\n",
    "Let's wrap this as a PyTorch layer now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3ce696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm_custom(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        # self.scale = torch.nn.Parameter(torch.randn())\n",
    "        # self.shift = torch.nn.Parameter(torch.randn())\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "\n",
    "        norm_tensor = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709161b7",
   "metadata": {},
   "source": [
    "Let's add this to the above FFN class and see if the outputs look normalized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c7a461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] n=15 x∈[-1.776, 1.646] μ=3.974e-09 σ=0.926"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ln = LayerNorm_custom()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.relu(self.linear(x))\n",
    "        o = self.ln(x)\n",
    "        return o\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547a5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm_custom2(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "\n",
    "        norm_tensor = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        output = self.scale * norm_tensor + self.shift\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c00fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] n=15 x∈[-1.776, 1.646] μ=3.974e-09 σ=0.926 grad DivBackward0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ln = LayerNorm_custom2(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.relu(self.linear(x))\n",
    "        o = self.ln(x)\n",
    "        return o\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5705a6",
   "metadata": {},
   "source": [
    "## GeLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073095ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[-1.151, 1.720] μ=1.325e-08 σ=0.939 grad DivBackward0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GeLU_custom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2) / torch.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        return o\n",
    "    \n",
    "\n",
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.act = GeLU_custom()\n",
    "        self.ln = LayerNorm_custom()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.linear(x)\n",
    "        o = self.act(o)\n",
    "        o = self.ln(o)\n",
    "        return o\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f0277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a4ff2a4",
   "metadata": {},
   "source": [
    "## Training loop with support for serializing and loading model and optimizer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd42ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b719acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.act = GeLU_custom()\n",
    "        self.ln = LayerNorm_custom()\n",
    "        self.linear2 = torch.nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = x @ self.linear\n",
    "        o = self.act(o)\n",
    "        o = self.ln(o)\n",
    "        o = o @ self.linear2\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28fee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(30, 5)\n",
    "labels = torch.randn(30)\n",
    "labels = torch.where(labels < 0.7, 0, 1)\n",
    "\n",
    "train_dataset = TensorDataset(data, labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d0c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  00/ 10 | train_loss:  1.8256\n",
      "epoch:  01/ 10 | train_loss:  1.8074\n",
      "epoch:  02/ 10 | train_loss:  1.7905\n",
      "epoch:  03/ 10 | train_loss:  1.7737\n",
      "epoch:  04/ 10 | train_loss:  1.7572\n",
      "epoch:  05/ 10 | train_loss:  1.7409\n",
      "epoch:  06/ 10 | train_loss:  1.7247\n",
      "epoch:  07/ 10 | train_loss:  1.7087\n",
      "epoch:  08/ 10 | train_loss:  1.6929\n",
      "epoch:  09/ 10 | train_loss:  1.6772\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = FFNWithNorm(5, 10, 1)\n",
    "\n",
    "# optimizer = AdamW_custom(model.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        logits = model(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = crossentropyloss(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    print(f\"epoch: {e: 03d}/{n_epochs: 03d} | train_loss: {avg_train_loss: .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1f39ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  00/ 10 | train_loss:  1.8256\n",
      "epoch:  01/ 10 | train_loss:  1.8074\n",
      "epoch:  02/ 10 | train_loss:  1.7905\n",
      "epoch:  03/ 10 | train_loss:  1.7737\n",
      "epoch:  04/ 10 | train_loss:  1.7572\n",
      "epoch:  05/ 10 | train_loss:  1.7409\n",
      "epoch:  06/ 10 | train_loss:  1.7247\n",
      "epoch:  07/ 10 | train_loss:  1.7087\n",
      "epoch:  08/ 10 | train_loss:  1.6929\n",
      "epoch:  09/ 10 | train_loss:  1.6772\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = FFNWithNorm(5, 10, 1)\n",
    "\n",
    "optimizer = AdamW_custom(params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    avg_train_loss = 0.0\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        logits = model(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = crossentropyloss(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    print(f\"epoch: {e: 03d}/{n_epochs: 03d} | train_loss: {avg_train_loss: .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc844e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear', tensor[5, 10] n=50 x∈[-2.132, 1.904] μ=0.099 σ=1.048),\n",
       "             ('linear2',\n",
       "              tensor[10, 1] x∈[-2.091, 1.252] μ=-0.066 σ=1.188 [[-0.572], [1.252], [-1.554], [-1.138], [0.864], [0.138], [-2.091], [0.925], [0.757], [0.762]])])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43872ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[5, 10] n=50 x∈[-2.132, 1.904] μ=0.099 σ=1.048"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e4f40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 1] x∈[-2.091, 1.252] μ=-0.066 σ=1.188 [[-0.572], [1.252], [-1.554], [-1.138], [0.864], [0.138], [-2.091], [0.925], [0.757], [0.762]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b5850e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea8af0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdamW_custom' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m() \u001b[38;5;66;03m# our custom implementation\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'AdamW_custom' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "optimizer.state_dict() # our custom implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae1319",
   "metadata": {},
   "source": [
    "Our optimizer needs to store all these details. Let's now modify our optimizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "439ca92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW_custom():#torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2):\n",
    "        # defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # super().__init__(params, defaults)\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.wd = weight_decay\n",
    "        self.eps = eps\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Initialize momentum and velocity for each parameter\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "        self.ckpt = {}\n",
    "        self.ckpt['param_groups'] = [{\n",
    "            'lr': lr,\n",
    "            'eps': eps,\n",
    "            'betas': (self.beta1, self.beta2),\n",
    "            'weight_decay': self.wd\n",
    "        }]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad = param.grad.data\n",
    "\n",
    "            # Apply weight decay directly to parameters (AdamW style)\n",
    "            param.data.mul_(1 - self.lr * self.wd)\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            # beta1 * i + (1-beta1) * grad\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad.pow(2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.step_count)\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.step_count)\n",
    "\n",
    "            # Update parameters\n",
    "            param.data.add_(m_hat * -self.lr/ (v_hat.sqrt() + self.eps))\n",
    "\n",
    "        \n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "029477dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  00/ 10 | train_loss:  1.8256\n",
      "epoch:  01/ 10 | train_loss:  1.8074\n",
      "epoch:  02/ 10 | train_loss:  1.7905\n",
      "epoch:  03/ 10 | train_loss:  1.7737\n",
      "epoch:  04/ 10 | train_loss:  1.7572\n",
      "epoch:  05/ 10 | train_loss:  1.7409\n",
      "epoch:  06/ 10 | train_loss:  1.7247\n",
      "epoch:  07/ 10 | train_loss:  1.7087\n",
      "epoch:  08/ 10 | train_loss:  1.6929\n",
      "epoch:  09/ 10 | train_loss:  1.6772\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = FFNWithNorm(5, 10, 1)\n",
    "\n",
    "optimizer = AdamW_custom(params=model.parameters())\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        logits = model(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = crossentropyloss(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    print(f\"epoch: {e: 03d}/{n_epochs: 03d} | train_loss: {avg_train_loss: .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c668aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_groups': [{'lr': 0.001,\n",
       "   'eps': 1e-08,\n",
       "   'betas': (0.9, 0.999),\n",
       "   'weight_decay': 0.01}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a620f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNWithNorm(\n",
       "  (act): GeLU_custom()\n",
       "  (ln): LayerNorm_custom()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04aea700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.act = GeLU_custom()\n",
    "        self.ln = LayerNorm_custom()\n",
    "        self.linear2 = torch.nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = x @ self.linear\n",
    "        o = self.act(o)\n",
    "        o = self.ln(o)\n",
    "        o = o @ self.linear2\n",
    "        return o\n",
    "    \n",
    "    def state_dict(self):\n",
    "        from collections import OrderedDict\n",
    "\n",
    "        sd = OrderedDict()\n",
    "        sd['linear'] = self.linear.data\n",
    "        sd['linear2'] = self.linear2.data\n",
    "        return sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783f28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  00/ 10 | train_loss:  1.8256\n",
      "epoch:  01/ 10 | train_loss:  1.8074\n",
      "epoch:  02/ 10 | train_loss:  1.7905\n",
      "epoch:  03/ 10 | train_loss:  1.7737\n",
      "epoch:  04/ 10 | train_loss:  1.7572\n",
      "epoch:  05/ 10 | train_loss:  1.7409\n",
      "epoch:  06/ 10 | train_loss:  1.7247\n",
      "epoch:  07/ 10 | train_loss:  1.7087\n",
      "epoch:  08/ 10 | train_loss:  1.6929\n",
      "epoch:  09/ 10 | train_loss:  1.6772\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = FFNWithNorm(5, 10, 1)\n",
    "\n",
    "optimizer = AdamW_custom(model.parameters())\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        logits = model(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = crossentropyloss(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    print(f\"epoch: {e: 03d}/{n_epochs: 03d} | train_loss: {avg_train_loss: .4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381218cd",
   "metadata": {},
   "source": [
    "## Serialize the model & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "007c4a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_state': OrderedDict([('linear',\n",
       "               tensor[5, 10] n=50 x∈[-2.132, 1.904] μ=0.099 σ=1.048),\n",
       "              ('linear2',\n",
       "               tensor[10, 1] x∈[-2.091, 1.252] μ=-0.066 σ=1.188 [[-0.572], [1.252], [-1.554], [-1.138], [0.864], [0.138], [-2.091], [0.925], [0.757], [0.762]])]),\n",
       " 'optimizer_state': {'param_groups': [{'lr': 0.001,\n",
       "    'eps': 1e-08,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'weight_decay': 0.01}]}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict()\n",
    "}\n",
    "\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "048229cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pth']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(ckpt, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcd6cf",
   "metadata": {},
   "source": [
    "## Load the model & optimizer checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67189a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_state': OrderedDict([('linear',\n",
       "               tensor[5, 10] n=50 x∈[-2.132, 1.904] μ=0.099 σ=1.048),\n",
       "              ('linear2',\n",
       "               tensor[10, 1] x∈[-2.091, 1.252] μ=-0.066 σ=1.188 [[-0.572], [1.252], [-1.554], [-1.138], [0.864], [0.138], [-2.091], [0.925], [0.757], [0.762]])]),\n",
       " 'optimizer_state': {'param_groups': [{'lr': 0.001,\n",
       "    'eps': 1e-08,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'weight_decay': 0.01}]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = joblib.load(\"model.pth\")\n",
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b531935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  00/ 10 | train_loss:  1.8256\n",
      "epoch:  01/ 10 | train_loss:  1.8074\n",
      "epoch:  02/ 10 | train_loss:  1.7905\n",
      "epoch:  03/ 10 | train_loss:  1.7737\n",
      "epoch:  04/ 10 | train_loss:  1.7572\n",
      "epoch:  05/ 10 | train_loss:  1.7409\n",
      "epoch:  06/ 10 | train_loss:  1.7247\n",
      "epoch:  07/ 10 | train_loss:  1.7087\n",
      "epoch:  08/ 10 | train_loss:  1.6929\n",
      "epoch:  09/ 10 | train_loss:  1.6772\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = FFNWithNorm(5, 10, 1)\n",
    "\n",
    "optimizer = AdamW_custom(model.parameters())\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "        logits = model(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = crossentropyloss(logits, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "\n",
    "    avg_train_loss = train_loss / train_batches\n",
    "\n",
    "    print(f\"epoch: {e: 03d}/{n_epochs: 03d} | train_loss: {avg_train_loss: .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb461986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd4907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
