{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312caa2e",
   "metadata": {},
   "source": [
    "# CS336 Assignments\n",
    "\n",
    "| # | Topic                         | Description                                 |\n",
    "|---|-------------------------------|---------------------------------------------|\n",
    "| 1 | Basics                        | Train an LLM from scratch                   |\n",
    "| 2 | Systems                       | Make it run fast!                           |\n",
    "| 3 | Scaling                       | Make it performant at a FLOP budget         |\n",
    "| 4 | Data                          | Prepare the right datasets                  |\n",
    "| 5 | Alignment & Reasoning RL      | Align it to real-world use cases            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4788fc7b",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "- Implement all of the components (tokenizer, model, loss function, optimizer) necessary to train a standard Transformer language model\n",
    "- Train a minimal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5564de0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phani/Work/Adhoc/projects/learning-ai/learn-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "tinystories = load_dataset(\"roneneldan/TinyStories\")\n",
    "tinystories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3600cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n",
       "  'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.',\n",
       "  'One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don\\'t want to play. I am cold and I don\\'t feel fine.\"\\n\\nFin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\\n\\nThe sun heard Fin\\'s call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don\\'t feel like I will freeze now. Let\\'s play together!\" And so, Fin and the crab played and became good friends.',\n",
       "  'Once upon a time, in a land full of trees, there was a little cherry tree. The cherry tree was very sad because it did not have any friends. All the other trees were big and strong, but the cherry tree was small and weak. The cherry tree was envious of the big trees.\\n\\nOne day, the cherry tree felt a tickle in its branches. It was a little spring wind. The wind told the cherry tree not to be sad. The wind said, \"You are special because you have sweet cherries that everyone loves.\" The cherry tree started to feel a little better.\\n\\nAs time went on, the cherry tree grew more and more cherries. All the animals in the land came to eat the cherries and play under the cherry tree. The cherry tree was happy because it had many friends now. The cherry tree learned that being different can be a good thing. And they all lived happily ever after.',\n",
       "  'Once upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog.\\n\\nOne day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there.\\n\\nLily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after.',\n",
       "  'Once upon a time, in a big lake, there was a brown kayak. The brown kayak liked to roll in the water all day long. It was very happy when it could roll and splash in the lake.\\n\\nOne day, a little boy named Tim came to play with the brown kayak. Tim and the brown kayak rolled in the water together. They laughed and had a lot of fun. The sun was shining, and the water was warm.\\n\\nAfter a while, it was time for Tim to go home. He said goodbye to the brown kayak and gave it a big hug. The brown kayak was sad to see Tim go, but it knew they would play together again soon. So, the brown kayak kept rolling in the water, waiting for the next fun day with Tim.',\n",
       "  'Once upon a time, in a small town, there was a troubled little girl named Lily. She was always sad because she lost her favorite toy, a triangle. She looked everywhere in her house but could not find it.\\n\\nOne sunny day, Lily went to the park to play. She saw a big puddle of water and thought her triangle might be there. She put her hand in the water to soak it and looked for her toy. She felt something at the bottom of the puddle.\\n\\nLily pulled it out and saw that it was her triangle! She was so happy that she found it. From that day on, Lily was never troubled again. She played with her triangle every day and always kept it close to her. And when she saw puddles, she would smile and remember how she found her toy.',\n",
       "  'Once upon a time, in a peaceful town, there lived a little boy named Tim. Tim loved to run and play outside. One day, Tim saw a race in the park. He was excited and wanted to join the race.\\n\\nTim went to his friend, Sarah, and said, \"Let\\'s start the race!\" Sarah smiled and said, \"Yes, let\\'s go!\" They lined up with the other kids and waited for the race to begin. When they heard the word \"Go!\", they started running as fast as they could.\\n\\nTim and Sarah ran with all their speed, laughing and having fun. They could feel the wind in their hair as they raced to the finish line. In the end, Tim won the race and Sarah came in second. They were both so happy and proud of themselves. They celebrated with their friends and had a great day at the park.',\n",
       "  'Once upon a time, there was a clever little dog named Max. Max loved to run and play with his friends in the park. One day, Max was running very fast when he fell and hurt his knee.\\n\\nMax went to his friend, the wise old owl, and said, \"Owl, my knee hurts. What can I do?\" The owl thought for a moment and said, \"Max, you should test your knee. Try to walk slowly and see if it still hurts.\"\\n\\nSo Max tested his knee by walking slowly. At first, it hurt a little, but soon Max felt better. He said, \"Thank you, Owl, for your help. Now I can play with my friends again.\"\\n\\nMax was so happy that he could play with his friends without pain. He learned that sometimes, it was good to slow down and listen to his body. And Max and his friends played happily in the park ever after.',\n",
       "  'One day, a fast driver named Tim went for a ride in his loud car. He loved to speed down the street and feel the wind in his hair. As he drove, he saw his friend, Sam, standing by the road.\\n\\n\"Hi, Sam!\" Tim called out. \"Do you want to go for a ride?\"\\n\\n\"Yes, please!\" Sam said, and he got in the car. They drove around the town, going fast and having fun. The car was very loud, and everyone could hear them coming.\\n\\nAt last, they stopped at the park to play. They ran and laughed until it was time to go home. Tim and Sam had a great day together, speeding in the loud car and playing in the park.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinystories['train'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4207a2b",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db9bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"Machine learning enables computers to learn from data.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Deep learning models require large amounts of data.\",\n",
    "    \"Neural networks are inspired by the human brain.\",\n",
    "    \"Data science combines statistics and computer science.\",\n",
    "    \"Transformers have revolutionized language modeling.\",\n",
    "    \"Open source software encourages collaboration.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c05061",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Steps to create a tokenizer:\n",
    "1. From all the words in our corpus, build a vocabulary\n",
    "2. Create a mapping between vocab and integer IDs\n",
    "3. Create a reverse mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673cca35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The', 'brown', 'dog.', 'fox', 'jumps', 'lazy', 'over', 'quick', 'the'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sample_text[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f2c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "words = ' '.join(sample_text).split()\n",
    "print(len(words))\n",
    "words = set(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04d2729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'combines': 0,\n",
       " 'have': 1,\n",
       " 'large': 2,\n",
       " 'to': 3,\n",
       " 'language': 4,\n",
       " 'The': 5,\n",
       " 'a': 6,\n",
       " 'by': 7,\n",
       " 'computer': 8,\n",
       " 'learn': 9,\n",
       " 'of': 10,\n",
       " 'Deep': 11,\n",
       " 'Machine': 12,\n",
       " 'from': 13,\n",
       " 'models': 14,\n",
       " 'Neural': 15,\n",
       " 'are': 16,\n",
       " 'the': 17,\n",
       " 'is': 18,\n",
       " 'human': 19,\n",
       " 'statistics': 20,\n",
       " 'encourages': 21,\n",
       " 'science.': 22,\n",
       " 'lazy': 23,\n",
       " 'science': 24,\n",
       " 'data.': 25,\n",
       " 'processing': 26,\n",
       " 'Data': 27,\n",
       " 'require': 28,\n",
       " 'jumps': 29,\n",
       " 'networks': 30,\n",
       " 'Transformers': 31,\n",
       " 'understand': 32,\n",
       " 'learning': 33,\n",
       " 'Natural': 34,\n",
       " 'brown': 35,\n",
       " 'transforming': 36,\n",
       " 'quick': 37,\n",
       " 'Artificial': 38,\n",
       " 'source': 39,\n",
       " 'helps': 40,\n",
       " 'world.': 41,\n",
       " 'amounts': 42,\n",
       " 'dog.': 43,\n",
       " 'enables': 44,\n",
       " 'popular': 45,\n",
       " 'Python': 46,\n",
       " 'brain.': 47,\n",
       " 'revolutionized': 48,\n",
       " 'programming': 49,\n",
       " 'collaboration.': 50,\n",
       " 'computers': 51,\n",
       " 'Open': 52,\n",
       " 'modeling.': 53,\n",
       " 'software': 54,\n",
       " 'language.': 55,\n",
       " 'fox': 56,\n",
       " 'and': 57,\n",
       " 'text.': 58,\n",
       " 'intelligence': 59,\n",
       " 'inspired': 60,\n",
       " 'over': 61}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s:i for i, s in enumerate(words)}\n",
    "itos = {i:s for i, s in enumerate(words)}\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "153c5f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 37, 35, 56, 29, 61, 17, 23, 43]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stoi[x] for x in sample_text[0].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11bdfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        self.vocab = ' '.join(sample_text).split()\n",
    "        self.stoi = {s:i for i, s in enumerate(self.vocab)}\n",
    "        self.itos = {i:s for i, s in enumerate(self.vocab)}\n",
    "        encoded_str = [stoi[x] for x in sample_text[0].split()]\n",
    "        return encoded_str\n",
    "\n",
    "    def decode(self, i: list[str]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf0e712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 37, 35, 56, 29, 61, 17, 23, 43]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "text_encoded = tok.encode(sample_text[0])\n",
    "text_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f27f46",
   "metadata": {},
   "source": [
    "Now, let's implement a decoder that takes in a list of integer IDs and returns the corresponding input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd521239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        self.vocab = ' '.join(sample_text).split()\n",
    "        self.stoi = {s:i for i, s in enumerate(self.vocab)}\n",
    "        self.itos = {i:s for i, s in enumerate(self.vocab)}\n",
    "        encoded_str = [stoi[x] for x in sample_text[0].split()]\n",
    "        return encoded_str\n",
    "\n",
    "    def decode(self, indices: list[str]):\n",
    "        decoded_str = [itos[i] for i in indices]\n",
    "        decoded_str = ' '.join(decoded_str) \n",
    "        return decoded_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04bb4b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "[5, 37, 35, 56, 29, 61, 17, 23, 43]\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "text_encoded = tok.encode(sample_text[0])\n",
    "\n",
    "print(sample_text[0])\n",
    "print(text_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff3f02c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(text_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977736be",
   "metadata": {},
   "source": [
    "While this works well, the vocab is created in runtime which is not desirable. The vocab should be create ahead of time so that any token can be encoded / decoded consistently. That means, we should create the vocab during initialization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "965d60ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2():\n",
    "    def __init__(self, text_corpus: list[str]):\n",
    "        self.text_corpus = text_corpus\n",
    "        self.vocab = set(' '.join(sample_text).split())\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = {s:i for i, s in enumerate(self.vocab)}\n",
    "        self.itos = {i:s for i, s in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        encoded_str = [stoi[x] for x in text.split()]\n",
    "        return encoded_str\n",
    "\n",
    "    def decode(self, indices: list[int]):\n",
    "        decoded_str = [itos[i] for i in indices]\n",
    "        decoded_str = ' '.join(decoded_str) \n",
    "        return decoded_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "385e5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 37, 35, 56, 29, 61, 17, 23, 43]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TokenizerV2(sample_text)\n",
    "tok.encode(sample_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd9cfd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'large learn Natural from programming combines and The of'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode([2, 9, 34, 13, 49, 0, 57, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b7e5cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 37, 35, 56, 29, 61, 17, 23, 43],\n",
       " [38, 59, 18, 36, 17, 41],\n",
       " [46, 18, 6, 45, 49, 55],\n",
       " [12, 33, 44, 51, 3, 9, 13, 25],\n",
       " [34, 4, 26, 40, 51, 32, 58],\n",
       " [11, 33, 14, 28, 2, 42, 10, 25],\n",
       " [15, 30, 16, 60, 7, 17, 19, 47],\n",
       " [27, 24, 0, 20, 57, 8, 22],\n",
       " [31, 1, 48, 4, 53],\n",
       " [52, 39, 54, 21, 50]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [tok.encode(s) for s in sample_text]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16eeb2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'Artificial intelligence is transforming the world.',\n",
       " 'Python is a popular programming language.',\n",
       " 'Machine learning enables computers to learn from data.',\n",
       " 'Natural language processing helps computers understand text.',\n",
       " 'Deep learning models require large amounts of data.',\n",
       " 'Neural networks are inspired by the human brain.',\n",
       " 'Data science combines statistics and computer science.',\n",
       " 'Transformers have revolutionized language modeling.',\n",
       " 'Open source software encourages collaboration.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.decode(i) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5a23515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'Artificial intelligence is transforming the world.',\n",
       " 'Python is a popular programming language.',\n",
       " 'Machine learning enables computers to learn from data.',\n",
       " 'Natural language processing helps computers understand text.',\n",
       " 'Deep learning models require large amounts of data.',\n",
       " 'Neural networks are inspired by the human brain.',\n",
       " 'Data science combines statistics and computer science.',\n",
       " 'Transformers have revolutionized language modeling.',\n",
       " 'Open source software encourages collaboration.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013cf4a",
   "metadata": {},
   "source": [
    "We are now able to convert text to integers and back. That's good. Let's try a new sentence with new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "360dfb61",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Satya'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSatya Nadella leads Microsoft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mTokenizerV2.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     encoded_str = [\u001b[43mstoi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m text.split()]\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_str\n",
      "\u001b[31mKeyError\u001b[39m: 'Satya'"
     ]
    }
   ],
   "source": [
    "tok.encode(\"Satya Nadella leads Microsoft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b26d4",
   "metadata": {},
   "source": [
    "As the word Satya is absent in the vocab, we see this error. This is common in real-world where a lot of new tokens can appear in the wild.\n",
    "\n",
    "We can add an unknown token during the vocab creation or use the more advanced BPE tokenizer which handles these situations quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a36951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 4, 15}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set([1,2,4])\n",
    "a.add(15)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22ad5676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV3():\n",
    "    def __init__(self, text_corpus: list[str]):\n",
    "        self.text_corpus = text_corpus\n",
    "        self.vocab = ' '.join(sample_text).split()\n",
    "        self.vocab = set(sorted(self.vocab))\n",
    "        self.vocab.add(\"<UNK>\")\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = {s:i for i, s in enumerate(self.vocab)}\n",
    "        self.itos = {i:s for i, s in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        encoded_str = [stoi.get(x, self.stoi['<UNK>']) for x in text.split()]\n",
    "        return encoded_str\n",
    "\n",
    "    def decode(self, indices: list[int]):\n",
    "        decoded_str = [self.itos.get(i) for i in indices]\n",
    "        decoded_str = ' '.join(decoded_str) \n",
    "        return decoded_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc7d2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 37, 35, 56, 29, 61, 17, 23, 43]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TokenizerV3(sample_text)\n",
    "tok.encode(sample_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd1293ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 32, 32, 17, 32]\n",
      "The <UNK> <UNK> human <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(tok.encode(\"The bird understand the dog\"))\n",
    "print(tok.decode(tok.encode(\"The bird understand the dog\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056a8b6",
   "metadata": {},
   "source": [
    "This is one way of handling unknown or special tokens. Now, we will build a BPE tokenizer which is a much better alternative for the following reasons:\n",
    "    1. Handle out of vocabulary tokens (and language nuances such as singulars and plurals effectively).\n",
    "\n",
    "This tokenizer breaks down words to subwords which are common in a language and this is shown to be more effective for language modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6139c6",
   "metadata": {},
   "source": [
    "# BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36706aa7",
   "metadata": {},
   "source": [
    "## Pseudo Algorithm for BPE (Byte Pair Encoding) Tokenizer\n",
    "(from github copilot)\n",
    "\n",
    "1. **Initialize Vocabulary**\n",
    "    - Start with a vocabulary of all unique characters in the corpus.\n",
    "\n",
    "2. **Tokenize Corpus**\n",
    "    - Split all words in the corpus into a list of characters (with a special end-of-word symbol, e.g., \"l o w </w>\").\n",
    "\n",
    "3. **Count Pair Frequencies**\n",
    "    - For all tokenized words, count the frequency of each adjacent character pair.\n",
    "\n",
    "4. **Merge Most Frequent Pair**\n",
    "    - Find the most frequent pair of characters.\n",
    "    - Merge this pair into a new token (e.g., \"l o\" → \"lo\").\n",
    "\n",
    "5. **Update Corpus**\n",
    "    - Replace all occurrences of the merged pair in the corpus with the new token.\n",
    "\n",
    "6. **Repeat**\n",
    "    - Repeat steps 3–5 for a predefined number of merges or until no pairs remain.\n",
    "\n",
    "7. **Build Final Vocabulary**\n",
    "    - The final vocabulary consists of all tokens created during the merge steps.\n",
    "\n",
    "8. **Tokenize New Text**\n",
    "    - For new input, iteratively apply the learned merges to split words into known tokens.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:**  \n",
    "- Add an `<UNK>` token for unknown words/subwords.\n",
    "- Store the merge operations for encoding/decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cb3cc",
   "metadata": {},
   "source": [
    "## initialize vocab & tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb2933f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    def __init__(self, text_corpus: list[str]):\n",
    "        self.vocab = set(' '.join(sample_text))\n",
    "        self.vocab.add(\"<UNK>\")\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        pass\n",
    "\n",
    "    def decode(self, indices: list[int]):\n",
    "        pass\n",
    "\n",
    "btok = BPETokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f65a52f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['f', 'k', 'w', 'j', 'i'], 36)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(btok.vocab)[:5], btok.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea07b1e",
   "metadata": {},
   "source": [
    "## count pair frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81883ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. Artificial intelligence is transforming the world. Python is a popular programming language. Machine learning enables computers to learn from data. Natural language processing helps computers understand text. Deep learning models require large amounts of data. Neural networks are inspired by the human brain. Data science combines statistics and computer science. Transformers have revolutionized language modeling. Open source software encourages collaboration.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(sample_text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf150e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e ': 15,\n",
       " 's ': 14,\n",
       " 'in': 11,\n",
       " '. ': 9,\n",
       " 'ng': 9,\n",
       " ' l': 8,\n",
       " 'ra': 8,\n",
       " 'an': 8,\n",
       " 'la': 7,\n",
       " 'ar': 7,\n",
       " 'er': 6,\n",
       " ' t': 6,\n",
       " 'ge': 6,\n",
       " 'en': 6,\n",
       " 'co': 6,\n",
       " 'at': 6,\n",
       " 're': 6,\n",
       " 'he': 5,\n",
       " 'n ': 5,\n",
       " 'ti': 5,\n",
       " 'te': 5,\n",
       " 'ce': 5,\n",
       " 'or': 5,\n",
       " 'g ': 5,\n",
       " ' c': 5,\n",
       " 'om': 5,\n",
       " 'ta': 5,\n",
       " ' s': 5,\n",
       " 'ro': 4,\n",
       " 'mp': 4,\n",
       " 'th': 4,\n",
       " ' i': 4,\n",
       " 'el': 4,\n",
       " 'nc': 4,\n",
       " ' a': 4,\n",
       " 'pu': 4,\n",
       " 'ag': 4,\n",
       " 'le': 4,\n",
       " 'es': 4,\n",
       " 'ut': 4,\n",
       " 'rs': 4,\n",
       " 'ur': 4,\n",
       " 'd ': 4,\n",
       " 'ic': 3,\n",
       " ' b': 3,\n",
       " 'fo': 3,\n",
       " 'r ': 3,\n",
       " ' d': 3,\n",
       " 'ci': 3,\n",
       " 'al': 3,\n",
       " 'l ': 3,\n",
       " 'is': 3,\n",
       " 'ns': 3,\n",
       " 'on': 3,\n",
       " ' p': 3,\n",
       " 'gu': 3,\n",
       " 'ua': 3,\n",
       " 'ne': 3,\n",
       " 'ea': 3,\n",
       " 'rn': 3,\n",
       " 'ni': 3,\n",
       " ' h': 3,\n",
       " 'nd': 3,\n",
       " 'de': 3,\n",
       " 'st': 3,\n",
       " 'mo': 3,\n",
       " 'ou': 3,\n",
       " 'qu': 2,\n",
       " 'ui': 2,\n",
       " 'br': 2,\n",
       " ' f': 2,\n",
       " 'um': 2,\n",
       " 'ps': 2,\n",
       " ' o': 2,\n",
       " 've': 2,\n",
       " 'y ': 2,\n",
       " 'og': 2,\n",
       " 'g.': 2,\n",
       " 'nt': 2,\n",
       " 'll': 2,\n",
       " 'li': 2,\n",
       " 'sf': 2,\n",
       " 'rm': 2,\n",
       " 'mi': 2,\n",
       " 'wo': 2,\n",
       " 'a ': 2,\n",
       " 'pr': 2,\n",
       " 'am': 2,\n",
       " 'e.': 2,\n",
       " ' e': 2,\n",
       " 'ab': 2,\n",
       " 'da': 2,\n",
       " 'a.': 2,\n",
       " ' N': 2,\n",
       " 'un': 2,\n",
       " ' D': 2,\n",
       " ' m': 2,\n",
       " 'od': 2,\n",
       " ' r': 2,\n",
       " 'ir': 2,\n",
       " 'of': 2,\n",
       " 'tw': 2,\n",
       " 'ed': 2,\n",
       " 'n.': 2,\n",
       " 'sc': 2,\n",
       " 'ie': 2,\n",
       " 'ol': 2,\n",
       " 'io': 2,\n",
       " 'so': 2,\n",
       " 'Th': 1,\n",
       " ' q': 1,\n",
       " 'ck': 1,\n",
       " 'k ': 1,\n",
       " 'ow': 1,\n",
       " 'wn': 1,\n",
       " 'ox': 1,\n",
       " 'x ': 1,\n",
       " ' j': 1,\n",
       " 'ju': 1,\n",
       " 'ov': 1,\n",
       " 'az': 1,\n",
       " 'zy': 1,\n",
       " 'do': 1,\n",
       " ' A': 1,\n",
       " 'Ar': 1,\n",
       " 'rt': 1,\n",
       " 'if': 1,\n",
       " 'fi': 1,\n",
       " 'ia': 1,\n",
       " 'ig': 1,\n",
       " 'tr': 1,\n",
       " ' w': 1,\n",
       " 'rl': 1,\n",
       " 'ld': 1,\n",
       " 'd.': 1,\n",
       " ' P': 1,\n",
       " 'Py': 1,\n",
       " 'yt': 1,\n",
       " 'ho': 1,\n",
       " 'po': 1,\n",
       " 'op': 1,\n",
       " 'ul': 1,\n",
       " 'gr': 1,\n",
       " 'mm': 1,\n",
       " ' M': 1,\n",
       " 'Ma': 1,\n",
       " 'ac': 1,\n",
       " 'ch': 1,\n",
       " 'hi': 1,\n",
       " 'na': 1,\n",
       " 'bl': 1,\n",
       " 'to': 1,\n",
       " 'o ': 1,\n",
       " 'fr': 1,\n",
       " 'm ': 1,\n",
       " 'Na': 1,\n",
       " 'tu': 1,\n",
       " 'oc': 1,\n",
       " 'ss': 1,\n",
       " 'si': 1,\n",
       " 'lp': 1,\n",
       " ' u': 1,\n",
       " 'ex': 1,\n",
       " 'xt': 1,\n",
       " 't.': 1,\n",
       " 'De': 1,\n",
       " 'ee': 1,\n",
       " 'ep': 1,\n",
       " 'p ': 1,\n",
       " 'ls': 1,\n",
       " 'eq': 1,\n",
       " 'rg': 1,\n",
       " 'ts': 1,\n",
       " 'f ': 1,\n",
       " 'Ne': 1,\n",
       " 'eu': 1,\n",
       " ' n': 1,\n",
       " 'et': 1,\n",
       " 'rk': 1,\n",
       " 'ks': 1,\n",
       " 'sp': 1,\n",
       " 'pi': 1,\n",
       " 'by': 1,\n",
       " 'hu': 1,\n",
       " 'ma': 1,\n",
       " 'ai': 1,\n",
       " 'Da': 1,\n",
       " 'mb': 1,\n",
       " 'bi': 1,\n",
       " 'cs': 1,\n",
       " ' T': 1,\n",
       " 'Tr': 1,\n",
       " 'me': 1,\n",
       " 'ha': 1,\n",
       " 'av': 1,\n",
       " 'ev': 1,\n",
       " 'vo': 1,\n",
       " 'lu': 1,\n",
       " 'iz': 1,\n",
       " 'ze': 1,\n",
       " ' O': 1,\n",
       " 'Op': 1,\n",
       " 'pe': 1,\n",
       " 'rc': 1,\n",
       " 'ft': 1,\n",
       " 'wa': 1,\n",
       " 'bo': 1,\n",
       " '.': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freq = {}\n",
    "for t, i in enumerate(text):\n",
    "    chars = text[t:t+2]\n",
    "    if chars in pair_freq:\n",
    "        pair_freq[chars] += 1\n",
    "    else:\n",
    "        pair_freq[chars] = 1\n",
    "\n",
    "sorted_pair_freq = dict(sorted(pair_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_pair_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd16cce",
   "metadata": {},
   "source": [
    "Let's remove single chars from the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09ddab05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. Artificial intelligence is transforming the world. Python is a popular programming language. Machine learning enables computers to learn from data. Natural language processing helps computers understand text. Deep learning models require large amounts of data. Neural networks are inspired by the human brain. Data science combines statistics and computer science. Transformers have revolutionized language modeling. Open source software encourages collaboration.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(sample_text)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bdb53fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 11,\n",
       " 'ng': 9,\n",
       " 'ra': 8,\n",
       " 'an': 8,\n",
       " 'la': 7,\n",
       " 'ar': 7,\n",
       " 'er': 6,\n",
       " 'ge': 6,\n",
       " 'en': 6,\n",
       " 'co': 6,\n",
       " 'at': 6,\n",
       " 're': 6,\n",
       " 'he': 5,\n",
       " 'ti': 5,\n",
       " 'te': 5,\n",
       " 'ce': 5,\n",
       " 'or': 5,\n",
       " 'om': 5,\n",
       " 'ta': 5,\n",
       " 'ro': 4,\n",
       " 'mp': 4,\n",
       " 'th': 4,\n",
       " 'el': 4,\n",
       " 'nc': 4,\n",
       " 'pu': 4,\n",
       " 'ag': 4,\n",
       " 'le': 4,\n",
       " 'es': 4,\n",
       " 'ut': 4,\n",
       " 'rs': 4,\n",
       " 'ur': 4,\n",
       " 'ic': 3,\n",
       " 'fo': 3,\n",
       " 'ci': 3,\n",
       " 'al': 3,\n",
       " 'is': 3,\n",
       " 'ns': 3,\n",
       " 'on': 3,\n",
       " 'gu': 3,\n",
       " 'ua': 3,\n",
       " 'ne': 3,\n",
       " 'ea': 3,\n",
       " 'rn': 3,\n",
       " 'ni': 3,\n",
       " 'nd': 3,\n",
       " 'de': 3,\n",
       " 'st': 3,\n",
       " 'mo': 3,\n",
       " 'ou': 3,\n",
       " 'qu': 2,\n",
       " 'ui': 2,\n",
       " 'br': 2,\n",
       " 'um': 2,\n",
       " 'ps': 2,\n",
       " 've': 2,\n",
       " 'og': 2,\n",
       " 'g.': 2,\n",
       " 'nt': 2,\n",
       " 'll': 2,\n",
       " 'li': 2,\n",
       " 'sf': 2,\n",
       " 'rm': 2,\n",
       " 'mi': 2,\n",
       " 'wo': 2,\n",
       " 'pr': 2,\n",
       " 'am': 2,\n",
       " 'e.': 2,\n",
       " 'ab': 2,\n",
       " 'da': 2,\n",
       " 'a.': 2,\n",
       " 'un': 2,\n",
       " 'od': 2,\n",
       " 'ir': 2,\n",
       " 'of': 2,\n",
       " 'tw': 2,\n",
       " 'ed': 2,\n",
       " 'n.': 2,\n",
       " 'sc': 2,\n",
       " 'ie': 2,\n",
       " 'ol': 2,\n",
       " 'io': 2,\n",
       " 'so': 2,\n",
       " 'Th': 1,\n",
       " 'ck': 1,\n",
       " 'ow': 1,\n",
       " 'wn': 1,\n",
       " 'ox': 1,\n",
       " 'ju': 1,\n",
       " 'ov': 1,\n",
       " 'az': 1,\n",
       " 'zy': 1,\n",
       " 'do': 1,\n",
       " 'Ar': 1,\n",
       " 'rt': 1,\n",
       " 'if': 1,\n",
       " 'fi': 1,\n",
       " 'ia': 1,\n",
       " 'ig': 1,\n",
       " 'tr': 1,\n",
       " 'rl': 1,\n",
       " 'ld': 1,\n",
       " 'd.': 1,\n",
       " 'Py': 1,\n",
       " 'yt': 1,\n",
       " 'ho': 1,\n",
       " 'po': 1,\n",
       " 'op': 1,\n",
       " 'ul': 1,\n",
       " 'gr': 1,\n",
       " 'mm': 1,\n",
       " 'Ma': 1,\n",
       " 'ac': 1,\n",
       " 'ch': 1,\n",
       " 'hi': 1,\n",
       " 'na': 1,\n",
       " 'bl': 1,\n",
       " 'to': 1,\n",
       " 'fr': 1,\n",
       " 'Na': 1,\n",
       " 'tu': 1,\n",
       " 'oc': 1,\n",
       " 'ss': 1,\n",
       " 'si': 1,\n",
       " 'lp': 1,\n",
       " 'ex': 1,\n",
       " 'xt': 1,\n",
       " 't.': 1,\n",
       " 'De': 1,\n",
       " 'ee': 1,\n",
       " 'ep': 1,\n",
       " 'ls': 1,\n",
       " 'eq': 1,\n",
       " 'rg': 1,\n",
       " 'ts': 1,\n",
       " 'Ne': 1,\n",
       " 'eu': 1,\n",
       " 'et': 1,\n",
       " 'rk': 1,\n",
       " 'ks': 1,\n",
       " 'sp': 1,\n",
       " 'pi': 1,\n",
       " 'by': 1,\n",
       " 'hu': 1,\n",
       " 'ma': 1,\n",
       " 'ai': 1,\n",
       " 'Da': 1,\n",
       " 'mb': 1,\n",
       " 'bi': 1,\n",
       " 'cs': 1,\n",
       " 'Tr': 1,\n",
       " 'me': 1,\n",
       " 'ha': 1,\n",
       " 'av': 1,\n",
       " 'ev': 1,\n",
       " 'vo': 1,\n",
       " 'lu': 1,\n",
       " 'iz': 1,\n",
       " 'ze': 1,\n",
       " 'Op': 1,\n",
       " 'pe': 1,\n",
       " 'rc': 1,\n",
       " 'ft': 1,\n",
       " 'wa': 1,\n",
       " 'bo': 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_freq = {}\n",
    "for t, i in enumerate(text):\n",
    "    chars = text[t:t+2]\n",
    "    if len(chars.strip()) == 1:\n",
    "        continue\n",
    "    if chars in pair_freq:\n",
    "        pair_freq[chars] += 1\n",
    "    else:\n",
    "        pair_freq[chars] = 1\n",
    "\n",
    "sorted_pair_freq = dict(sorted(pair_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_pair_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444938bf",
   "metadata": {},
   "source": [
    "## merge most frequent pair\n",
    "\n",
    "let's consider cutoff as 5 for merging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97e2672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog. Artificial intelligence is transforming the world. Python is a popular programming language. Machine learning enables computers to learn from data. Natural language processing helps computers understand text. Deep learning models require large amounts of data. Neural networks are inspired by the human brain. Data science combines statistics and computer science. Transformers have revolutionized language modeling. Open source software encourages collaboration.\n",
      "['f', 'k', 'w', 'j', 'i', 'a', 'y', 'N', 'T', 'p', 'o', 'x', 's', 'n', 'D', 'v', 't', 'z', 'l', '.', 'A', ' ', 'r', 'O', 'c', 'M', 'u', 'P', 'h', 'b', 'g', 'd', 'q', 'e', 'm']\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(sample_text)\n",
    "print(text)\n",
    "\n",
    "chars = list(set(' '.join(sample_text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60529511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e73a51e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 11,\n",
       " 'ng': 9,\n",
       " 'ra': 8,\n",
       " 'an': 8,\n",
       " 'la': 7,\n",
       " 'ar': 7,\n",
       " 'er': 6,\n",
       " 'ge': 6,\n",
       " 'en': 6,\n",
       " 'co': 6,\n",
       " 'at': 6,\n",
       " 're': 6,\n",
       " 'he': 5,\n",
       " 'ti': 5,\n",
       " 'te': 5,\n",
       " 'ce': 5,\n",
       " 'or': 5,\n",
       " 'om': 5,\n",
       " 'ta': 5,\n",
       " 'ro': 4,\n",
       " 'mp': 4,\n",
       " 'th': 4,\n",
       " 'el': 4,\n",
       " 'nc': 4,\n",
       " 'pu': 4,\n",
       " 'ag': 4,\n",
       " 'le': 4,\n",
       " 'es': 4,\n",
       " 'ut': 4,\n",
       " 'rs': 4,\n",
       " 'ur': 4,\n",
       " 'ic': 3,\n",
       " 'fo': 3,\n",
       " 'ci': 3,\n",
       " 'al': 3,\n",
       " 'is': 3,\n",
       " 'ns': 3,\n",
       " 'on': 3,\n",
       " 'gu': 3,\n",
       " 'ua': 3,\n",
       " 'ne': 3,\n",
       " 'ea': 3,\n",
       " 'rn': 3,\n",
       " 'ni': 3,\n",
       " 'nd': 3,\n",
       " 'de': 3,\n",
       " 'st': 3,\n",
       " 'mo': 3,\n",
       " 'ou': 3,\n",
       " 'qu': 2,\n",
       " 'ui': 2,\n",
       " 'br': 2,\n",
       " 'um': 2,\n",
       " 'ps': 2,\n",
       " 've': 2,\n",
       " 'og': 2,\n",
       " 'g.': 2,\n",
       " 'nt': 2,\n",
       " 'll': 2,\n",
       " 'li': 2,\n",
       " 'sf': 2,\n",
       " 'rm': 2,\n",
       " 'mi': 2,\n",
       " 'wo': 2,\n",
       " 'pr': 2,\n",
       " 'am': 2,\n",
       " 'e.': 2,\n",
       " 'ab': 2,\n",
       " 'da': 2,\n",
       " 'a.': 2,\n",
       " 'un': 2,\n",
       " 'od': 2,\n",
       " 'ir': 2,\n",
       " 'of': 2,\n",
       " 'tw': 2,\n",
       " 'ed': 2,\n",
       " 'n.': 2,\n",
       " 'sc': 2,\n",
       " 'ie': 2,\n",
       " 'ol': 2,\n",
       " 'io': 2,\n",
       " 'so': 2,\n",
       " 'Th': 1,\n",
       " 'ck': 1,\n",
       " 'ow': 1,\n",
       " 'wn': 1,\n",
       " 'ox': 1,\n",
       " 'ju': 1,\n",
       " 'ov': 1,\n",
       " 'az': 1,\n",
       " 'zy': 1,\n",
       " 'do': 1,\n",
       " 'Ar': 1,\n",
       " 'rt': 1,\n",
       " 'if': 1,\n",
       " 'fi': 1,\n",
       " 'ia': 1,\n",
       " 'ig': 1,\n",
       " 'tr': 1,\n",
       " 'rl': 1,\n",
       " 'ld': 1,\n",
       " 'd.': 1,\n",
       " 'Py': 1,\n",
       " 'yt': 1,\n",
       " 'ho': 1,\n",
       " 'po': 1,\n",
       " 'op': 1,\n",
       " 'ul': 1,\n",
       " 'gr': 1,\n",
       " 'mm': 1,\n",
       " 'Ma': 1,\n",
       " 'ac': 1,\n",
       " 'ch': 1,\n",
       " 'hi': 1,\n",
       " 'na': 1,\n",
       " 'bl': 1,\n",
       " 'to': 1,\n",
       " 'fr': 1,\n",
       " 'Na': 1,\n",
       " 'tu': 1,\n",
       " 'oc': 1,\n",
       " 'ss': 1,\n",
       " 'si': 1,\n",
       " 'lp': 1,\n",
       " 'ex': 1,\n",
       " 'xt': 1,\n",
       " 't.': 1,\n",
       " 'De': 1,\n",
       " 'ee': 1,\n",
       " 'ep': 1,\n",
       " 'ls': 1,\n",
       " 'eq': 1,\n",
       " 'rg': 1,\n",
       " 'ts': 1,\n",
       " 'Ne': 1,\n",
       " 'eu': 1,\n",
       " 'et': 1,\n",
       " 'rk': 1,\n",
       " 'ks': 1,\n",
       " 'sp': 1,\n",
       " 'pi': 1,\n",
       " 'by': 1,\n",
       " 'hu': 1,\n",
       " 'ma': 1,\n",
       " 'ai': 1,\n",
       " 'Da': 1,\n",
       " 'mb': 1,\n",
       " 'bi': 1,\n",
       " 'cs': 1,\n",
       " 'Tr': 1,\n",
       " 'me': 1,\n",
       " 'ha': 1,\n",
       " 'av': 1,\n",
       " 'ev': 1,\n",
       " 'vo': 1,\n",
       " 'lu': 1,\n",
       " 'iz': 1,\n",
       " 'ze': 1,\n",
       " 'Op': 1,\n",
       " 'pe': 1,\n",
       " 'rc': 1,\n",
       " 'ft': 1,\n",
       " 'wa': 1,\n",
       " 'bo': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = chars\n",
    "pair_freq = {}\n",
    "for t, i in enumerate(text):\n",
    "    char_group = text[t:t+2]\n",
    "    if len(char_group.strip()) == 1:\n",
    "        continue\n",
    "    if char_group in pair_freq:\n",
    "        pair_freq[char_group] += 1\n",
    "        if pair_freq[char_group] >= 5:\n",
    "            vocab.append(char_group)\n",
    "    else:\n",
    "        pair_freq[char_group] = 1\n",
    "\n",
    "sorted_pair_freq = dict(sorted(pair_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7006e2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog. Artificial intelligence is transforming the world. Python is a popular programming language. Machine learning enables computers to learn from data. Natural language processing helps computers understand text. Deep learning models require large amounts of data. Neural networks are inspired by the human brain. Data science combines statistics and computer science. Transformers have revolutionized language modeling. Open source software encourages collaboration.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d13ddfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 11,\n",
       " 'ng': 9,\n",
       " 'ra': 8,\n",
       " 'an': 8,\n",
       " 'la': 7,\n",
       " 'ar': 7,\n",
       " 'er': 6,\n",
       " 'ge': 6,\n",
       " 'en': 6,\n",
       " 'co': 6,\n",
       " 'at': 6,\n",
       " 're': 6,\n",
       " 'ing': 6,\n",
       " 'he': 5,\n",
       " 'ti': 5,\n",
       " 'te': 5,\n",
       " 'ce': 5,\n",
       " 'or': 5,\n",
       " 'om': 5,\n",
       " 'ta': 5,\n",
       " 'ro': 4,\n",
       " 'mp': 4,\n",
       " 'th': 4,\n",
       " 'el': 4,\n",
       " 'nc': 4,\n",
       " 'pu': 4,\n",
       " 'ag': 4,\n",
       " 'le': 4,\n",
       " 'es': 4,\n",
       " 'ut': 4,\n",
       " 'rs': 4,\n",
       " 'ur': 4,\n",
       " 'enc': 4,\n",
       " 'age': 4,\n",
       " 'com': 4,\n",
       " 'ers': 4,\n",
       " 'ic': 3,\n",
       " 'fo': 3,\n",
       " 'ci': 3,\n",
       " 'al': 3,\n",
       " 'is': 3,\n",
       " 'ns': 3,\n",
       " 'on': 3,\n",
       " 'gu': 3,\n",
       " 'ua': 3,\n",
       " 'ne': 3,\n",
       " 'ea': 3,\n",
       " 'rn': 3,\n",
       " 'ni': 3,\n",
       " 'nd': 3,\n",
       " 'de': 3,\n",
       " 'st': 3,\n",
       " 'mo': 3,\n",
       " 'ou': 3,\n",
       " 'the': 3,\n",
       " 'nce': 3,\n",
       " 'lan': 3,\n",
       " 'ang': 3,\n",
       " 'ngu': 3,\n",
       " 'gua': 3,\n",
       " 'uag': 3,\n",
       " 'lea': 3,\n",
       " 'ear': 3,\n",
       " 'arn': 3,\n",
       " 'omp': 3,\n",
       " 'mpu': 3,\n",
       " 'put': 3,\n",
       " 'ute': 3,\n",
       " 'ter': 3,\n",
       " 'ata': 3,\n",
       " 'ura': 3,\n",
       " 'qu': 2,\n",
       " 'ui': 2,\n",
       " 'br': 2,\n",
       " 'um': 2,\n",
       " 'ps': 2,\n",
       " 've': 2,\n",
       " 'og': 2,\n",
       " 'g.': 2,\n",
       " 'nt': 2,\n",
       " 'll': 2,\n",
       " 'li': 2,\n",
       " 'sf': 2,\n",
       " 'rm': 2,\n",
       " 'mi': 2,\n",
       " 'wo': 2,\n",
       " 'pr': 2,\n",
       " 'am': 2,\n",
       " 'e.': 2,\n",
       " 'ab': 2,\n",
       " 'da': 2,\n",
       " 'a.': 2,\n",
       " 'un': 2,\n",
       " 'od': 2,\n",
       " 'ir': 2,\n",
       " 'of': 2,\n",
       " 'tw': 2,\n",
       " 'ed': 2,\n",
       " 'n.': 2,\n",
       " 'sc': 2,\n",
       " 'ie': 2,\n",
       " 'ol': 2,\n",
       " 'io': 2,\n",
       " 'so': 2,\n",
       " 'qui': 2,\n",
       " 'ran': 2,\n",
       " 'ans': 2,\n",
       " 'nsf': 2,\n",
       " 'sfo': 2,\n",
       " 'for': 2,\n",
       " 'orm': 2,\n",
       " 'min': 2,\n",
       " 'wor': 2,\n",
       " 'lar': 2,\n",
       " 'pro': 2,\n",
       " 'ine': 2,\n",
       " 'rni': 2,\n",
       " 'nin': 2,\n",
       " 'dat': 2,\n",
       " 'ta.': 2,\n",
       " 'ral': 2,\n",
       " 'sta': 2,\n",
       " 'and': 2,\n",
       " 'mod': 2,\n",
       " 'ode': 2,\n",
       " 'del': 2,\n",
       " 'ire': 2,\n",
       " 'are': 2,\n",
       " 'sci': 2,\n",
       " 'cie': 2,\n",
       " 'ien': 2,\n",
       " 'ati': 2,\n",
       " 'tio': 2,\n",
       " 'ion': 2,\n",
       " 'our': 2,\n",
       " 'Th': 1,\n",
       " 'ck': 1,\n",
       " 'ow': 1,\n",
       " 'wn': 1,\n",
       " 'ox': 1,\n",
       " 'ju': 1,\n",
       " 'ov': 1,\n",
       " 'az': 1,\n",
       " 'zy': 1,\n",
       " 'do': 1,\n",
       " 'Ar': 1,\n",
       " 'rt': 1,\n",
       " 'if': 1,\n",
       " 'fi': 1,\n",
       " 'ia': 1,\n",
       " 'ig': 1,\n",
       " 'tr': 1,\n",
       " 'rl': 1,\n",
       " 'ld': 1,\n",
       " 'd.': 1,\n",
       " 'Py': 1,\n",
       " 'yt': 1,\n",
       " 'ho': 1,\n",
       " 'po': 1,\n",
       " 'op': 1,\n",
       " 'ul': 1,\n",
       " 'gr': 1,\n",
       " 'mm': 1,\n",
       " 'Ma': 1,\n",
       " 'ac': 1,\n",
       " 'ch': 1,\n",
       " 'hi': 1,\n",
       " 'na': 1,\n",
       " 'bl': 1,\n",
       " 'to': 1,\n",
       " 'fr': 1,\n",
       " 'Na': 1,\n",
       " 'tu': 1,\n",
       " 'oc': 1,\n",
       " 'ss': 1,\n",
       " 'si': 1,\n",
       " 'lp': 1,\n",
       " 'ex': 1,\n",
       " 'xt': 1,\n",
       " 't.': 1,\n",
       " 'De': 1,\n",
       " 'ee': 1,\n",
       " 'ep': 1,\n",
       " 'ls': 1,\n",
       " 'eq': 1,\n",
       " 'rg': 1,\n",
       " 'ts': 1,\n",
       " 'Ne': 1,\n",
       " 'eu': 1,\n",
       " 'et': 1,\n",
       " 'rk': 1,\n",
       " 'ks': 1,\n",
       " 'sp': 1,\n",
       " 'pi': 1,\n",
       " 'by': 1,\n",
       " 'hu': 1,\n",
       " 'ma': 1,\n",
       " 'ai': 1,\n",
       " 'Da': 1,\n",
       " 'mb': 1,\n",
       " 'bi': 1,\n",
       " 'cs': 1,\n",
       " 'Tr': 1,\n",
       " 'me': 1,\n",
       " 'ha': 1,\n",
       " 'av': 1,\n",
       " 'ev': 1,\n",
       " 'vo': 1,\n",
       " 'lu': 1,\n",
       " 'iz': 1,\n",
       " 'ze': 1,\n",
       " 'Op': 1,\n",
       " 'pe': 1,\n",
       " 'rc': 1,\n",
       " 'ft': 1,\n",
       " 'wa': 1,\n",
       " 'bo': 1,\n",
       " 'The': 1,\n",
       " 'uic': 1,\n",
       " 'ick': 1,\n",
       " 'bro': 1,\n",
       " 'row': 1,\n",
       " 'own': 1,\n",
       " 'fox': 1,\n",
       " 'jum': 1,\n",
       " 'ump': 1,\n",
       " 'mps': 1,\n",
       " 'ove': 1,\n",
       " 'ver': 1,\n",
       " 'laz': 1,\n",
       " 'azy': 1,\n",
       " 'dog': 1,\n",
       " 'og.': 1,\n",
       " 'Art': 1,\n",
       " 'rti': 1,\n",
       " 'tif': 1,\n",
       " 'ifi': 1,\n",
       " 'fic': 1,\n",
       " 'ici': 1,\n",
       " 'cia': 1,\n",
       " 'ial': 1,\n",
       " 'int': 1,\n",
       " 'nte': 1,\n",
       " 'tel': 1,\n",
       " 'ell': 1,\n",
       " 'lli': 1,\n",
       " 'lig': 1,\n",
       " 'ige': 1,\n",
       " 'gen': 1,\n",
       " 'tra': 1,\n",
       " 'rmi': 1,\n",
       " 'orl': 1,\n",
       " 'rld': 1,\n",
       " 'ld.': 1,\n",
       " 'Pyt': 1,\n",
       " 'yth': 1,\n",
       " 'tho': 1,\n",
       " 'hon': 1,\n",
       " ' a ': 1,\n",
       " 'pop': 1,\n",
       " 'opu': 1,\n",
       " 'pul': 1,\n",
       " 'ula': 1,\n",
       " 'rog': 1,\n",
       " 'ogr': 1,\n",
       " 'gra': 1,\n",
       " 'ram': 1,\n",
       " 'amm': 1,\n",
       " 'mmi': 1,\n",
       " 'ge.': 1,\n",
       " 'Mac': 1,\n",
       " 'ach': 1,\n",
       " 'chi': 1,\n",
       " 'hin': 1,\n",
       " 'ena': 1,\n",
       " 'nab': 1,\n",
       " 'abl': 1,\n",
       " 'ble': 1,\n",
       " 'les': 1,\n",
       " 'fro': 1,\n",
       " 'rom': 1,\n",
       " 'Nat': 1,\n",
       " 'atu': 1,\n",
       " 'tur': 1,\n",
       " 'roc': 1,\n",
       " 'oce': 1,\n",
       " 'ces': 1,\n",
       " 'ess': 1,\n",
       " 'ssi': 1,\n",
       " 'sin': 1,\n",
       " 'hel': 1,\n",
       " 'elp': 1,\n",
       " 'lps': 1,\n",
       " 'und': 1,\n",
       " 'nde': 1,\n",
       " 'der': 1,\n",
       " 'rst': 1,\n",
       " 'tan': 1,\n",
       " 'tex': 1,\n",
       " 'ext': 1,\n",
       " 'xt.': 1,\n",
       " 'Dee': 1,\n",
       " 'eep': 1,\n",
       " 'els': 1,\n",
       " 'req': 1,\n",
       " 'equ': 1,\n",
       " 'uir': 1,\n",
       " 'arg': 1,\n",
       " 'rge': 1,\n",
       " 'amo': 1,\n",
       " 'mou': 1,\n",
       " 'oun': 1,\n",
       " 'unt': 1,\n",
       " 'nts': 1,\n",
       " 'Neu': 1,\n",
       " 'eur': 1,\n",
       " 'net': 1,\n",
       " 'etw': 1,\n",
       " 'two': 1,\n",
       " 'ork': 1,\n",
       " 'rks': 1,\n",
       " 'ins': 1,\n",
       " 'nsp': 1,\n",
       " 'spi': 1,\n",
       " 'pir': 1,\n",
       " 'red': 1,\n",
       " 'hum': 1,\n",
       " 'uma': 1,\n",
       " 'man': 1,\n",
       " 'bra': 1,\n",
       " 'rai': 1,\n",
       " 'ain': 1,\n",
       " 'in.': 1,\n",
       " 'Dat': 1,\n",
       " 'omb': 1,\n",
       " 'mbi': 1,\n",
       " 'bin': 1,\n",
       " 'nes': 1,\n",
       " 'tat': 1,\n",
       " 'tis': 1,\n",
       " 'ist': 1,\n",
       " 'sti': 1,\n",
       " 'tic': 1,\n",
       " 'ics': 1,\n",
       " 'ce.': 1,\n",
       " 'Tra': 1,\n",
       " 'rme': 1,\n",
       " 'mer': 1,\n",
       " 'hav': 1,\n",
       " 'ave': 1,\n",
       " 'rev': 1,\n",
       " 'evo': 1,\n",
       " 'vol': 1,\n",
       " 'olu': 1,\n",
       " 'lut': 1,\n",
       " 'uti': 1,\n",
       " 'oni': 1,\n",
       " 'niz': 1,\n",
       " 'ize': 1,\n",
       " 'zed': 1,\n",
       " 'eli': 1,\n",
       " 'lin': 1,\n",
       " 'ng.': 1,\n",
       " 'Ope': 1,\n",
       " 'pen': 1,\n",
       " 'sou': 1,\n",
       " 'urc': 1,\n",
       " 'rce': 1,\n",
       " 'sof': 1,\n",
       " 'oft': 1,\n",
       " 'ftw': 1,\n",
       " 'twa': 1,\n",
       " 'war': 1,\n",
       " 'nco': 1,\n",
       " 'cou': 1,\n",
       " 'rag': 1,\n",
       " 'ges': 1,\n",
       " 'col': 1,\n",
       " 'oll': 1,\n",
       " 'lla': 1,\n",
       " 'lab': 1,\n",
       " 'abo': 1,\n",
       " 'bor': 1,\n",
       " 'ora': 1,\n",
       " 'rat': 1,\n",
       " 'on.': 1,\n",
       " '.': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab = chars\n",
    "for t, i in enumerate(text):\n",
    "    char_group = text[t:t+3]\n",
    "    if len(char_group.replace(\" \", \"\")) == 2:\n",
    "        continue\n",
    "    if char_group in pair_freq:\n",
    "        pair_freq[char_group] += 1\n",
    "        if pair_freq[char_group] >= 5:\n",
    "            vocab.append(char_group)\n",
    "    else:\n",
    "        pair_freq[char_group] = 1\n",
    "\n",
    "sorted_pair_freq = dict(sorted(pair_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_pair_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2a60c",
   "metadata": {},
   "source": [
    "Now that the new bigram tokens are in our vocab, continue doing this until there are no more tokens left under the target frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9371e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227bf680",
   "metadata": {},
   "source": [
    "In practice, we use the efficient BPETokenizer implementation from tiktoken library from OpenAI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842156f0",
   "metadata": {},
   "source": [
    "# Tiktoken BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4e21482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "ttok = tiktoken.get_encoding(\"gpt2\")\n",
    "ttok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94a2c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original  : Hey! I am doing well.. I hope you're well too! \n",
      "encoded   : [10814, 0, 314, 716, 1804, 880, 492, 314, 2911, 345, 821, 880, 1165, 0, 220]\n",
      "decoded   : Hey! I am doing well.. I hope you're well too! \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hey! I am doing well.. I hope you're well too! \"\n",
    "print(f\"original  : {sentence}\")\n",
    "\n",
    "sentence_encoded = ttok.encode(sentence)\n",
    "print(f\"encoded   : {sentence_encoded}\")\n",
    "\n",
    "sentence_decoded = ttok.decode(sentence_encoded)\n",
    "print(f\"decoded   : {sentence_decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e10dfae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttok = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "ttok.encode(tinystories['train'][0]['text'])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb966b",
   "metadata": {},
   "source": [
    "Let's now convert an entire batch of text into integerIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f31460ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttok.encode_batch(tinystories['train'][0:8]['text'])[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9847975d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[162, 177, 212, 193, 159, 168, 171, 181]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(t) for t in ttok.encode_batch(tinystories['train'][0:8]['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa3eca",
   "metadata": {},
   "source": [
    "We can convert this into a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8ca406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ids = ttok.encode_batch(tinystories['train'][0:8]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a5af459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "         17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "           284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "          2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "           673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "           198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "           366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "          2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "          1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "           356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "           198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "         19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "           407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "          1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "          1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "          1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "          1978,    13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   612,   373,   257,  1310,  1097,\n",
       "          3706,  1355,   538,    13,  1355,   538,  6151,   284,   467,  3049,\n",
       "           290,   711,   287,   262,  4252,    13,  1355,   538,   373,   257,\n",
       "          5448,  1097,   780,   339,  1464,   550,   922,  5252,    13,  4599,\n",
       "          5252,   925,  1355,   538,  3772,   290,  1913,    13,   198,   198,\n",
       "          3198,  1110,    11,  1355,   538,   373,  5059,   287,   262,  3952,\n",
       "           618,   339,  2497,   257,  1263,  5509,    13,   383,  5509,   550,\n",
       "           867,  5667,   326,   547,  7463,    13,  1355,   538,  8288,   703,\n",
       "           262,  5667,  2121,   290,  2227,   284,   711,   351,   606,    13,\n",
       "          1355,   538, 10357,   739,   262,  5509,   290,  7342,   262,  5667,\n",
       "          2121,   319,   683,    13,   679, 13818,   290,   307,   538,   276,\n",
       "           465, 12718,    13,   198,   198,  3856,   538,  2826,   351,   262,\n",
       "          7463,  5667,   477,  1110,    13,  1649,   340,   373,   640,   284,\n",
       "           467,  1363,    11,  1355,   538,  2993,   339,  2622,   517,  5252,\n",
       "            13,   679,  1816,   284,   262,  5252,  1295,   290,  1392,   517,\n",
       "          5448,  5252,    13,  2735,    11,  1355,   538,   373,  3492,   284,\n",
       "           467,  3049,   290,   711,   757,   262,  1306,  1110,    13,   843,\n",
       "          1355,   538,  5615, 18177,  1683,   706,    13]),\n",
       " tensor([ 3198,  1110,    11,   257,  1310,  5916,  3706,  4463,   373, 14899,\n",
       "          1474,   262, 15191,    13,   679,  2497,   257,  1263, 32202,   290,\n",
       "          2227,   284,   307,  2460,    13,   366, 17250,    11,   314,   716,\n",
       "          4463,    13,  2141,   345,   765,   284,   711,  1701,  1965,   262,\n",
       "          1310,  5916,    13,   383, 32202,  3114,   379,  4463,   290,   531,\n",
       "            11,   366,  2949,    11,   314,   836,   470,   765,   284,   711,\n",
       "            13,   314,   716,  4692,   290,   314,   836,   470,  1254,  3734,\n",
       "           526,   198,   198, 18467,  2936,  6507,   475,  2227,   284,  1037,\n",
       "           262, 32202,  1254,  1365,    13,   679,  1509,   321,  1497,   290,\n",
       "          1807,   286,   257,  1410,    13,   679, 12086,   326,   262,  4252,\n",
       "           714,   787,  1243,  5814,    13,  1406,    11,  4463,  1509,   321,\n",
       "           284,   262,  1353,   286,   262,  1660,   290,  1444,   284,   262,\n",
       "          4252,    11,   366,  5492,    11,  4252,    11,  1037,   616,   649,\n",
       "          1545,  1254,  3734,   290,   407, 16611,  2474,   198,   198,   464,\n",
       "          4252,  2982,  4463,   338,   869,   290, 44193,   663,  5814,  1657,\n",
       "           319,   262, 15191,    13,   383, 32202,  2067,   284,  1254,  1365,\n",
       "           290,   407,   523,  4692,    13,   679,  2497,  4463,   290,   531,\n",
       "            11,   366, 10449,   345,    11,  1310,  5916,    11,   329,  1642,\n",
       "           502,  1254,  3734,    13,   314,   836,   470,  1254,   588,   314,\n",
       "           481, 16611,   783,    13,  3914,   338,   711,  1978,  2474,   843,\n",
       "           523,    11,  4463,   290,   262, 32202,  2826,   290,  2627,   922,\n",
       "          2460,    13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   287,   257,  1956,  1336,   286,\n",
       "          7150,    11,   612,   373,   257,  1310, 23612,  5509,    13,   383,\n",
       "         23612,  5509,   373,   845,  6507,   780,   340,   750,   407,   423,\n",
       "           597,  2460,    13,  1439,   262,   584,  7150,   547,  1263,   290,\n",
       "          1913,    11,   475,   262, 23612,  5509,   373,  1402,   290,  4939,\n",
       "            13,   383, 23612,  5509,   373,   551,  1442,   286,   262,  1263,\n",
       "          7150,    13,   198,   198,  3198,  1110,    11,   262, 23612,  5509,\n",
       "          2936,   257,  4378,   293,   287,   663, 13737,    13,   632,   373,\n",
       "           257,  1310,  6076,  2344,    13,   383,  2344,  1297,   262, 23612,\n",
       "          5509,   407,   284,   307,  6507,    13,   383,  2344,   531,    11,\n",
       "           366,  1639,   389,  2041,   780,   345,   423,  6029, 21382,  1678,\n",
       "           326,  2506, 10408,   526,   383, 23612,  5509,  2067,   284,  1254,\n",
       "           257,  1310,  1365,    13,   198,   198,  1722,   640,  1816,   319,\n",
       "            11,   262, 23612,  5509,  6348,   517,   290,   517, 21382,  1678,\n",
       "            13,  1439,   262,  4695,   287,   262,  1956,  1625,   284,  4483,\n",
       "           262, 21382,  1678,   290,   711,   739,   262, 23612,  5509,    13,\n",
       "           383, 23612,  5509,   373,  3772,   780,   340,   550,   867,  2460,\n",
       "           783,    13,   383, 23612,  5509,  4499,   326,   852,  1180,   460,\n",
       "           307,   257,   922,  1517,    13,   843,   484,   477,  5615, 18177,\n",
       "          1683,   706,    13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   612,   373,   257,  1310,  2576,\n",
       "          3706, 20037,    13, 20037,  8288,   284, 16614,   673,   373,   257,\n",
       "          2968, 21752,    13,  1375,  5615,   287,   257,  1263, 16669,   351,\n",
       "           607,  1266,  2460,    11,   257,  3797,   290,   257,  3290,    13,\n",
       "           198,   198,  3198,  1110,    11,   981,  2712,   287,   262, 16669,\n",
       "            11, 20037,  1043,   257,  1263, 22843, 12384,    13,   383, 22843,\n",
       "         12384,   373,   287,   262,   835,   286,   607,  1257,   983,    13,\n",
       "          1375,  2227,   284,   651,  5755,   286,   340,    11,   475,   673,\n",
       "           373, 12008,   286,   262, 19230,   326,  5615,   612,    13,   198,\n",
       "           198,    43,   813,  1965,   607,  2460,    11,   262,  3797,   290,\n",
       "           262,  3290,    11,   284,  1037,   607,    13,  1119,   477,  3111,\n",
       "          1978,   284,  3424,   262, 22843, 12384,    13,   383, 19230,   373,\n",
       "          6507,    11,   475,   340,  1043,   257,   649,  1363,  2354,    13,\n",
       "         20037,    11,   262,  3797,    11,   290,   262,  3290,   547,  3772,\n",
       "           484,   714,   711,  1231,   262, 22843, 12384,   287,   262,   835,\n",
       "            13,   843,   484,   477,  5615, 18177,  1683,   706,    13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   287,   257,  1263, 13546,    11,\n",
       "           612,   373,   257,  7586, 34681,   461,    13,   383,  7586, 34681,\n",
       "           461,  8288,   284,  4836,   287,   262,  1660,   477,  1110,   890,\n",
       "            13,   632,   373,   845,  3772,   618,   340,   714,  4836,   290,\n",
       "         22870,   287,   262, 13546,    13,   198,   198,  3198,  1110,    11,\n",
       "           257,  1310,  2933,  3706,  5045,  1625,   284,   711,   351,   262,\n",
       "          7586, 34681,   461,    13,  5045,   290,   262,  7586, 34681,   461,\n",
       "         11686,   287,   262,  1660,  1978,    13,  1119, 13818,   290,   550,\n",
       "           257,  1256,   286,  1257,    13,   383,  4252,   373, 22751,    11,\n",
       "           290,   262,  1660,   373,  5814,    13,   198,   198,  3260,   257,\n",
       "           981,    11,   340,   373,   640,   329,  5045,   284,   467,  1363,\n",
       "            13,   679,   531, 24829,   284,   262,  7586, 34681,   461,   290,\n",
       "          2921,   340,   257,  1263, 16225,    13,   383,  7586, 34681,   461,\n",
       "           373,  6507,   284,   766,  5045,   467,    11,   475,   340,  2993,\n",
       "           484,   561,   711,  1978,   757,  2582,    13,  1406,    11,   262,\n",
       "          7586, 34681,   461,  4030, 10708,   287,   262,  1660,    11,  4953,\n",
       "           329,   262,  1306,  1257,  1110,   351,  5045,    13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   287,   257,  1402,  3240,    11,\n",
       "           612,   373,   257, 17840,  1310,  2576,  3706, 20037,    13,  1375,\n",
       "           373,  1464,  6507,   780,   673,  2626,   607,  4004, 13373,    11,\n",
       "           257, 22950,    13,  1375,  3114,  8347,   287,   607,  2156,   475,\n",
       "           714,   407,  1064,   340,    13,   198,   198,  3198, 27737,  1110,\n",
       "            11, 20037,  1816,   284,   262,  3952,   284,   711,    13,  1375,\n",
       "          2497,   257,  1263,   279, 24500,   286,  1660,   290,  1807,   607,\n",
       "         22950,  1244,   307,   612,    13,  1375,  1234,   607,  1021,   287,\n",
       "           262,  1660,   284, 33182,   340,   290,  3114,   329,   607, 13373,\n",
       "            13,  1375,  2936,  1223,   379,   262,  4220,   286,   262,   279,\n",
       "         24500,    13,   198,   198,    43,   813,  5954,   340,   503,   290,\n",
       "          2497,   326,   340,   373,   607, 22950,     0,  1375,   373,   523,\n",
       "          3772,   326,   673,  1043,   340,    13,  3574,   326,  1110,   319,\n",
       "            11, 20037,   373,  1239, 17840,   757,    13,  1375,  2826,   351,\n",
       "           607, 22950,   790,  1110,   290,  1464,  4030,   340,  1969,   284,\n",
       "           607,    13,   843,   618,   673,  2497,   279,  4185,   829,    11,\n",
       "           673,   561,  8212,   290,  3505,   703,   673,  1043,   607, 13373,\n",
       "            13]),\n",
       " tensor([ 7454,  2402,   257,   640,    11,   287,   257, 12309,  3240,    11,\n",
       "           612,  5615,   257,  1310,  2933,  3706,  5045,    13,  5045,  6151,\n",
       "           284,  1057,   290,   711,  2354,    13,  1881,  1110,    11,  5045,\n",
       "          2497,   257,  3234,   287,   262,  3952,    13,   679,   373,  6568,\n",
       "           290,  2227,   284,  4654,   262,  3234,    13,   198,   198, 14967,\n",
       "          1816,   284,   465,  1545,    11, 10490,    11,   290,   531,    11,\n",
       "           366,  5756,   338,   923,   262,  3234,  2474, 10490, 13541,   290,\n",
       "           531,    11,   366,  5297,    11,  1309,   338,   467,  2474,  1119,\n",
       "         16566,   510,   351,   262,   584,  3988,   290, 13488,   329,   262,\n",
       "          3234,   284,  2221,    13,  1649,   484,  2982,   262,  1573,   366,\n",
       "          5247, 40754,   484,  2067,  2491,   355,  3049,   355,   484,   714,\n",
       "            13,   198,   198, 14967,   290, 10490,  4966,   351,   477,   511,\n",
       "          2866,    11, 14376,   290,  1719,  1257,    13,  1119,   714,  1254,\n",
       "           262,  2344,   287,   511,  4190,   355,   484, 32897,   284,   262,\n",
       "          5461,  1627,    13,   554,   262,   886,    11,  5045,  1839,   262,\n",
       "          3234,   290, 10490,  1625,   287,  1218,    13,  1119,   547,  1111,\n",
       "           523,  3772,   290,  6613,   286,  2405,    13,  1119, 13943,   351,\n",
       "           511,  2460,   290,   550,   257,  1049,  1110,   379,   262,  3952,\n",
       "            13])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.tensor(x) for x in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5ac32",
   "metadata": {},
   "source": [
    "To convert this into a tensor with padding, we will be using the `torch.nn.utils.rnn.pad_sequence` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2fd6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3198, 1110,   11,  ...,    0,    0,    0],\n",
       "        [7454, 2402,  257,  ...,    0,    0,    0],\n",
       "        [3198, 1110,   11,  ...,  922, 2460,   13],\n",
       "        ...,\n",
       "        [7454, 2402,  257,  ...,    0,    0,    0],\n",
       "        [7454, 2402,  257,  ...,    0,    0,    0],\n",
       "        [7454, 2402,  257,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tids = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in ids], batch_first=True)\n",
    "tids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1368c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
