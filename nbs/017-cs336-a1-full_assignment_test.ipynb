{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bca9b",
   "metadata": {},
   "source": [
    "# CS336 Assignments\n",
    "\n",
    "| # | Topic                         | Description                                 |\n",
    "|---|-------------------------------|---------------------------------------------|\n",
    "| 1 | Basics                        | Train an LLM from scratch                   |\n",
    "| 2 | Systems                       | Make it run fast!                           |\n",
    "| 3 | Scaling                       | Make it performant at a FLOP budget         |\n",
    "| 4 | Data                          | Prepare the right datasets                  |\n",
    "| 5 | Alignment & Reasoning RL      | Align it to real-world use cases            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f19a5c",
   "metadata": {},
   "source": [
    "### Assignment #1\n",
    "- Implement all of the components (tokenizer, model, loss function, optimizer) necessary to train a standard Transformer language model\n",
    "- Train a minimal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fad44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from datasets import load_dataset\n",
    "import joblib\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29cb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import ascii_letters\n",
    "\n",
    "{l: ord(l) for l in ascii_letters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82da92ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(115)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd26ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Problem (unicode1): Understanding Unicode (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92f1886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842e52",
   "metadata": {},
   "source": [
    "This represents a null character often used to represent end of a string. It is also called an escape sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd338fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr('\\x00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382b1b51",
   "metadata": {},
   "source": [
    "The string representation of this character is '\\x00'. When this string is passed to print function, it's rendered as null as that is the purpose of this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb1d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1bdc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edeebc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12691eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5f80c",
   "metadata": {},
   "source": [
    "When we print the character with the print function, the character is executed and hence renders nothing on the stdout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32a387",
   "metadata": {},
   "source": [
    "### Exercise 2: Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fddecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_string_encoded: b'Hello'\n",
      "Byte values: [72, 101, 108, 108, 111]\n",
      "test string decoded: Hello\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Hello\"\n",
    "test_string_encoded = test_string.encode(\"UTF-8\")\n",
    "print(f\"test_string_encoded: {test_string_encoded}\")\n",
    "print(f\"Byte values: {list(test_string_encoded)}\")\n",
    "print(f\"test string decoded: {test_string_encoded.decode(\"UTF-8\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bca8fe",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings\n",
    "\n",
    "A: Majority of the internet comprises of UTF-8 characters. And, UTF-8 is space efficient as 5 characters in UTF-8 takes 5 bytes whereas UTF-16 and UTF-32 takes 2x and 4x the bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b7879d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcafé\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"café\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f465a",
   "metadata": {},
   "source": [
    "The function attempts to convert each character as a standalone single character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc26adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'caf\\xc3\\xa9'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"café\".encode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93e0a7",
   "metadata": {},
   "source": [
    "## Exercise 3: Problem (train_bpe): BPE Tokenizer Training (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eaf49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(input_path, vocab_size=1000, special_tokens=[]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaef1b9",
   "metadata": {},
   "source": [
    "## Exercise 4: Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9f3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tinystories(input_path, vocab_size=10000, special_tokens=[\"|endoftext|\"]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e844a74",
   "metadata": {},
   "source": [
    "## Exercise 5: Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647fe501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_expts_owt(input_path, vocab_size=32000, special_tokens=[\"|endoftext|\"]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01767cd1",
   "metadata": {},
   "source": [
    "## Exercise 6: Problem (tokenizer): Implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fd29c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokkenizer():\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        pass\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        pass\n",
    "\n",
    "    def decode(self, ids:list[str]):\n",
    "        pass\n",
    "\n",
    "    def from_files():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3652ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2771b5a7",
   "metadata": {},
   "source": [
    "## Exercise 7: Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68591549",
   "metadata": {},
   "source": [
    "a. Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867e2c53",
   "metadata": {},
   "source": [
    "b. What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com\u0002pare the compression ratio and/or qualitatively describe what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c007a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d881be8b",
   "metadata": {},
   "source": [
    "## Exercise 8: Problem (linear): Implementing the linear module (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c7714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 2] n=6 x∈[-1.725, 0.063] μ=-0.740 σ=0.702 grad ViewBackward0 [[-1.281, -1.725], [-0.531, -0.048], [0.063, -0.920]]\n",
       "tensor([[-1.2815, -1.7247],\n",
       "        [-0.5314, -0.0480],\n",
       "        [ 0.0631, -0.9196]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        weight = torch.empty(in_features, out_features)\n",
    "        sigma = sqrt(2 / (in_features + out_features))\n",
    "        torch.nn.init.trunc_normal_(weight, mean=0, std=sigma, a=-3 * sigma, b=3 * sigma)\n",
    "\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        # output = x @ self.W \n",
    "\n",
    "        # einsum way\n",
    "        output = einsum(x, self.W, \"... j, j k-> ... k\")\n",
    "\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyLinear(5, 2)\n",
    "\n",
    "batch = torch.randn(3, 5)\n",
    "\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cee67",
   "metadata": {},
   "source": [
    "## Exercise 9: Problem (embedding): Implement the embedding module (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "146775a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[4] i64 x∈[0, 9] μ=4.500 σ=5.196 [0, 9, 0, 9]\n",
      "tensor([0, 9, 0, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[4, 5] n=20 x∈[-0.856, 1.729] μ=0.398 σ=0.862 grad IndexBackward0\n",
       "tensor([[ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557],\n",
       "        [ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        weight = torch.empty(num_embeddings, embedding_dim)\n",
    "        torch.nn.init.trunc_normal_(weight, mean=0, std=1, a=-3, b=3)\n",
    "\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        output = self.W[x]\n",
    "\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyEmbedding(10, 5)\n",
    "\n",
    "batch = torch.randint(0, 10, (4, ))\n",
    "\n",
    "print(batch.v)\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f610c2d",
   "metadata": {},
   "source": [
    "## Exercise 10: Problem (rmsnorm): Root Mean Square Layer Normalization (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a95b8ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[-1.502, 1.712] μ=0.232 σ=0.988 grad MulBackward0\n",
       "tensor([[ 1.3741,  1.0606,  0.6423, -1.5015,  0.4838, -0.8804, -0.0307, -1.1443],\n",
       "        [-0.7808,  1.7116, -0.4074, -1.4571, -0.7556, -0.5807, -0.7981,  0.7915],\n",
       "        [ 1.6059, -0.1561, -0.4864,  0.4298, -0.7413,  1.0544,  0.7831,  1.6434],\n",
       "        [ 1.4381,  1.4575,  0.6863,  1.5006, -0.2604,  0.0469, -0.2828,  0.9667]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyRMSNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        weight = torch.ones(d_model)\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        output = x.to(torch.float32)\n",
    "        denom = torch.sqrt(output.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        output = output / denom\n",
    "        output = output * self.W\n",
    "        output = output.to(self.dtype)\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyRMSNorm(8)\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00f6fd",
   "metadata": {},
   "source": [
    "## Exercise 11: Problem (positionwise_feedforward): Implement the position-wise feed-forward network (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09b37cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[-0.278, 1.682] μ=0.360 σ=0.627\n",
       "tensor([[ 1.6820,  1.2131,  0.6405, -0.2286,  0.4501, -0.2783, -0.0211, -0.2685],\n",
       "        [-0.2410,  1.3828, -0.1582, -0.2769, -0.2370, -0.2035, -0.2435,  0.5199],\n",
       "        [ 1.3760, -0.0734, -0.1881,  0.2673, -0.2419,  0.8046,  0.5527,  1.4167],\n",
       "        [ 1.0007,  1.0180,  0.3956,  1.0566, -0.1025,  0.0213, -0.1100,  0.6042]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MySilu(torch.nn.Module):\n",
    "    def __init__(self, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        sigm = torch.sigmoid(x)\n",
    "        output = x * sigm\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MySilu()\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d0273",
   "metadata": {},
   "source": [
    "## Exercise 12: Problem (rope): Implement RoPE (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a63e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from math import sqrt\n",
    "# from einops import einsum\n",
    "\n",
    "# class MyRope(torch.nn.Module):\n",
    "#     def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None, ):\n",
    "#         super().__init__()\n",
    "#         self.device = device\n",
    "#         self.dtype = dtype\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # PyTorch way\n",
    "#         sigm = torch.sigmoid(x)\n",
    "#         output = x * sigm\n",
    "#         return output\n",
    "    \n",
    "# torch.manual_seed(42)\n",
    "# model = MySilu()\n",
    "\n",
    "# batch = torch.randn((4, 8))\n",
    "# output = model(batch)\n",
    "# output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6a2fc",
   "metadata": {},
   "source": [
    "## Exercise 13: Problem (softmax): Implement softmax (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12f5579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[0.007, 0.507] μ=0.125 σ=0.119\n",
       "tensor([[0.3971, 0.2558, 0.1423, 0.0070, 0.1139, 0.0168, 0.0554, 0.0116],\n",
       "        [0.0460, 0.5071, 0.0659, 0.0240, 0.0471, 0.0557, 0.0452, 0.2090],\n",
       "        [0.2693, 0.0444, 0.0317, 0.0809, 0.0244, 0.1532, 0.1161, 0.2799],\n",
       "        [0.2011, 0.2046, 0.1031, 0.2126, 0.0444, 0.0584, 0.0435, 0.1323]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MySoftmax(x: torch.Tensor, dim: int):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # numerical stability at high values of logits\n",
    "    # dim-wise max and not overall max\n",
    "    max = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    x = x - max\n",
    "\n",
    "    # usual business here\n",
    "    num = torch.exp(x)\n",
    "    denom = torch.exp(x).sum(dim=dim, keepdim=True)\n",
    "    output = num / (denom + eps)\n",
    "    return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "output = MySoftmax(batch, -1)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaeb2dd",
   "metadata": {},
   "source": [
    "## Exercise 14: Problem (scaled_dot_product_attention): Implement scaled dot-product attention (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a01b1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 4, 8] n=64 x∈[-1.856, 1.337] μ=-0.164 σ=0.848 \u001b[31mNaN!\u001b[0m\n",
       "tensor([[[-1.1559, -0.6587,  0.9698, -0.5012,  0.0382, -0.6236, -0.4751,\n",
       "           0.1088],\n",
       "         [-1.7076, -1.1538,  1.0961,  0.0806,  0.7386, -1.2000, -1.5364,\n",
       "           1.0948],\n",
       "         [ 0.5950,  0.1390,  0.9795,  1.2782, -0.6123, -0.2495, -0.5716,\n",
       "           0.4881],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan]],\n",
       "\n",
       "        [[-0.2903, -0.9661,  0.3084, -0.0689, -0.3157, -0.3652,  0.5390,\n",
       "          -0.8634],\n",
       "         [-0.2998, -0.6605,  0.3965,  0.5087,  1.3371, -0.3157,  0.8563,\n",
       "          -1.2335],\n",
       "         [-0.0314, -0.8524, -1.3506, -0.8969,  1.3004, -1.8561,  0.5581,\n",
       "          -0.4879],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def Myscaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    #  ## einsum approach\n",
    "    attn_scores = einsum(Q, K, \"... queries d_k, ... keys d_k -> ... queries keys\")\n",
    "    attn_weights = attn_scores / torch.sqrt(torch.tensor(Q.shape[-1]))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "    attn_weights = MySoftmax(attn_weights, dim=-1)\n",
    "    context_vec = einsum(attn_weights, V, \"... queries sl, ... sl d_v -> ... queries d_v\")\n",
    "    return context_vec\n",
    "    \n",
    "torch.manual_seed(420)\n",
    "\n",
    "Q = torch.randn((2, 4, 8))\n",
    "K = torch.randn((2, 4, 8))\n",
    "V = torch.randn((2, 4, 8))\n",
    "\n",
    "# creating a mask\n",
    "mask = torch.randn((4, 4))\n",
    "mask = torch.triu(mask, diagonal=1).to(bool)\n",
    "\n",
    "output = Myscaled_dot_product_attention(Q, K, V, mask)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30452c",
   "metadata": {},
   "source": [
    "## Exercise 15: Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d08208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[6, 8] n=48 x∈[-0.719, 2.601] μ=0.160 σ=0.580 grad CatBackward0\n",
       "tensor([[-0.2432,  0.3059,  0.1249, -0.2378,  0.3230,  0.1088, -0.3383,  0.2205],\n",
       "        [-0.1074,  0.2105,  1.6652,  0.4818,  0.0650,  0.5881,  0.0076, -0.1414],\n",
       "        [-0.2060,  0.3413,  1.1676,  0.4955, -0.0630, -0.5109, -0.7194, -0.5144],\n",
       "        [-0.1612,  0.3109,  2.6013,  0.9274,  0.0252,  0.2998, -0.0720,  0.1192],\n",
       "        [-0.2827,  0.1304, -0.0213, -0.0715, -0.0075,  0.4589, -0.1612, -0.3944],\n",
       "        [-0.2487,  0.3305,  1.1002,  0.5451, -0.1859, -0.3870, -0.2347,  0.0226]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySDPA(torch.nn.Module):\n",
    "    def __init__(self, seq_len, emb_size):\n",
    "        super().__init__()\n",
    "        self.W_query = torch.nn.Parameter(torch.randn(seq_len, emb_size))\n",
    "        self.W_key   = torch.nn.Parameter(torch.randn(seq_len, emb_size))\n",
    "        self.W_value = torch.nn.Parameter(torch.randn(seq_len, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return Myscaled_dot_product_attention(self.W_query, self.W_key, self.W_value, None)\n",
    "\n",
    "class MyCausalMHA(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, seq_len: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.emb_size = emb_size\n",
    "        self.d_k = self.d_model / self.num_heads\n",
    "\n",
    "        self.heads = torch.nn.ModuleList([MySDPA(self.seq_len, self.emb_size)\n",
    "                                         for _ in range(self.num_heads)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        context_vecs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = MyMHA(d_model=8, num_heads=2, seq_len=6, emb_size=4)\n",
    "batch = torch.randn((2, 6, 4))\n",
    "context_vec = model(batch)\n",
    "context_vec.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441a193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddade89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
