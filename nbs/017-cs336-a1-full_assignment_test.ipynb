{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bca9b",
   "metadata": {},
   "source": [
    "# CS336 Assignments\n",
    "\n",
    "| # | Topic                         | Description                                 |\n",
    "|---|-------------------------------|---------------------------------------------|\n",
    "| 1 | Basics                        | Train an LLM from scratch                   |\n",
    "| 2 | Systems                       | Make it run fast!                           |\n",
    "| 3 | Scaling                       | Make it performant at a FLOP budget         |\n",
    "| 4 | Data                          | Prepare the right datasets                  |\n",
    "| 5 | Alignment & Reasoning RL      | Align it to real-world use cases            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f19a5c",
   "metadata": {},
   "source": [
    "### Assignment #1\n",
    "- Implement all of the components (tokenizer, model, loss function, optimizer) necessary to train a standard Transformer language model\n",
    "- Train a minimal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fad44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from datasets import load_dataset\n",
    "import joblib\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f29cb95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import ascii_letters\n",
    "\n",
    "{l: ord(l) for l in ascii_letters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82da92ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(115)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd26ea",
   "metadata": {},
   "source": [
    "### Exercise 1: Problem (unicode1): Understanding Unicode (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92f1886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81842e52",
   "metadata": {},
   "source": [
    "This represents a null character often used to represent end of a string. It is also called an escape sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd338fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr('\\x00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cc439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382b1b51",
   "metadata": {},
   "source": [
    "The string representation of this character is '\\x00'. When this string is passed to print function, it's rendered as null as that is the purpose of this character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb1d013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1bdc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edeebc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d12691eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a5f80c",
   "metadata": {},
   "source": [
    "When we print the character with the print function, the character is executed and hence renders nothing on the stdout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32a387",
   "metadata": {},
   "source": [
    "### Exercise 2: Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fddecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_string_encoded: b'Hello'\n",
      "Byte values: [72, 101, 108, 108, 111]\n",
      "test string decoded: Hello\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Hello\"\n",
    "test_string_encoded = test_string.encode(\"UTF-8\")\n",
    "print(f\"test_string_encoded: {test_string_encoded}\")\n",
    "print(f\"Byte values: {list(test_string_encoded)}\")\n",
    "print(f\"test string decoded: {test_string_encoded.decode(\"UTF-8\")}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bca8fe",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings\n",
    "\n",
    "A: Majority of the internet comprises of UTF-8 characters. And, UTF-8 is space efficient as 5 characters in UTF-8 takes 5 bytes whereas UTF-16 and UTF-32 takes 2x and 4x the bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b7879d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcafé\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mdecode_utf8_bytes_to_str_wrong\u001b[39m\u001b[34m(bytestring)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"café\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f465a",
   "metadata": {},
   "source": [
    "The function attempts to convert each character as a standalone single character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc26adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'caf\\xc3\\xa9'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"café\".encode(\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c93e0a7",
   "metadata": {},
   "source": [
    "## Exercise 3: Problem (train_bpe): BPE Tokenizer Training (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eaf49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(input_path, vocab_size=1000, special_tokens=[]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaef1b9",
   "metadata": {},
   "source": [
    "## Exercise 4: Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf9f3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tinystories(input_path, vocab_size=10000, special_tokens=[\"|endoftext|\"]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e844a74",
   "metadata": {},
   "source": [
    "## Exercise 5: Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647fe501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_expts_owt(input_path, vocab_size=32000, special_tokens=[\"|endoftext|\"]):\n",
    "    vocab, merges = None, None\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01767cd1",
   "metadata": {},
   "source": [
    "## Exercise 6: Problem (tokenizer): Implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd29c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokkenizer():\n",
    "    def __init__(self, vocab, merges, special_tokens=None):\n",
    "        pass\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        pass\n",
    "\n",
    "    def decode(self, ids:list[str]):\n",
    "        pass\n",
    "\n",
    "    def from_files():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3652ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2771b5a7",
   "metadata": {},
   "source": [
    "## Exercise 7: Problem (tokenizer_experiments): Experiments with tokenizers (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68591549",
   "metadata": {},
   "source": [
    "a. Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1a20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "867e2c53",
   "metadata": {},
   "source": [
    "b. What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com\u0002pare the compression ratio and/or qualitatively describe what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c007a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d881be8b",
   "metadata": {},
   "source": [
    "## Exercise 8: Problem (linear): Implementing the linear module (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c7714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 2] n=6 x∈[-1.725, 0.063] μ=-0.740 σ=0.702 grad ViewBackward0 [[-1.281, -1.725], [-0.531, -0.048], [0.063, -0.920]]\n",
       "tensor([[-1.2815, -1.7247],\n",
       "        [-0.5314, -0.0480],\n",
       "        [ 0.0631, -0.9196]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        weight = torch.empty(in_features, out_features)\n",
    "        sigma = sqrt(2 / (in_features + out_features))\n",
    "        torch.nn.init.trunc_normal_(weight, mean=0, std=sigma, a=-3 * sigma, b=3 * sigma)\n",
    "\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        # output = x @ self.W \n",
    "\n",
    "        # einsum way\n",
    "        output = einsum(x, self.W, \"... j, j k-> ... k\")\n",
    "\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyLinear(5, 2)\n",
    "\n",
    "batch = torch.randn(3, 5)\n",
    "\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103cee67",
   "metadata": {},
   "source": [
    "## Exercise 9: Problem (embedding): Implement the embedding module (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146775a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[4] i64 x∈[0, 9] μ=4.500 σ=5.196 [0, 9, 0, 9]\n",
      "tensor([0, 9, 0, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[4, 5] n=20 x∈[-0.856, 1.729] μ=0.398 σ=0.862 grad IndexBackward0\n",
       "tensor([[ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557],\n",
       "        [ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        weight = torch.empty(num_embeddings, embedding_dim)\n",
    "        torch.nn.init.trunc_normal_(weight, mean=0, std=1, a=-3, b=3)\n",
    "\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        output = self.W[x]\n",
    "\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyEmbedding(10, 5)\n",
    "\n",
    "batch = torch.randint(0, 10, (4, ))\n",
    "\n",
    "print(batch.v)\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67df26e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[4] i64 x∈[0, 9] μ=4.500 σ=5.196 [0, 9, 0, 9]\n",
      "tensor([0, 9, 0, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor[4, 5] n=20 x∈[-0.856, 1.729] μ=0.398 σ=0.862 grad IndexBackward0\n",
       "tensor([[ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557],\n",
       "        [ 1.1812,  1.3651, -0.2971,  1.7287, -0.2774],\n",
       "        [-0.5769,  0.7990,  0.2255,  0.6847, -0.8557]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        weight = torch.empty(num_embeddings, embedding_dim)\n",
    "        torch.nn.init.trunc_normal_(weight, mean=0, std=1, a=-3, b=3)\n",
    "\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        output = self.W[x]\n",
    "\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MyEmbedding(10, 5)\n",
    "\n",
    "batch = torch.randint(0, 10, (4, ))\n",
    "\n",
    "print(batch.v)\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f610c2d",
   "metadata": {},
   "source": [
    "## Exercise 10: Problem (rmsnorm): Root Mean Square Layer Normalization (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a95b8ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[-1.502, 1.712] μ=0.232 σ=0.988 grad MulBackward0\n",
       "tensor([[ 1.3741,  1.0606,  0.6423, -1.5015,  0.4838, -0.8804, -0.0307, -1.1443],\n",
       "        [-0.7808,  1.7116, -0.4074, -1.4571, -0.7556, -0.5807, -0.7981,  0.7915],\n",
       "        [ 1.6059, -0.1561, -0.4864,  0.4298, -0.7413,  1.0544,  0.7831,  1.6434],\n",
       "        [ 1.4381,  1.4575,  0.6863,  1.5006, -0.2604,  0.0469, -0.2828,  0.9667]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MyRMSNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        weight = torch.ones(d_model)\n",
    "        self.W = torch.nn.Parameter(weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        output = x.to(torch.float32)\n",
    "        denom = torch.sqrt(output.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        output = output / denom\n",
    "        output = output * self.W\n",
    "        output = output.to(self.dtype)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = MyRMSNorm(8)\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00f6fd",
   "metadata": {},
   "source": [
    "## Exercise 11: Problem (positionwise_feedforward): Implement the position-wise feed-forward network (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09b37cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[-0.278, 1.682] μ=0.360 σ=0.627\n",
       "tensor([[ 1.6820,  1.2131,  0.6405, -0.2286,  0.4501, -0.2783, -0.0211, -0.2685],\n",
       "        [-0.2410,  1.3828, -0.1582, -0.2769, -0.2370, -0.2035, -0.2435,  0.5199],\n",
       "        [ 1.3760, -0.0734, -0.1881,  0.2673, -0.2419,  0.8046,  0.5527,  1.4167],\n",
       "        [ 1.0007,  1.0180,  0.3956,  1.0566, -0.1025,  0.0213, -0.1100,  0.6042]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "from einops import einsum\n",
    "\n",
    "class MySilu(torch.nn.Module):\n",
    "    def __init__(self, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        sigm = torch.sigmoid(x)\n",
    "        output = x * sigm\n",
    "        return output\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MySilu()\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2de58d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 6] n=24 x∈[-74.395, 109.831] μ=3.797 σ=42.568\n",
       "tensor([[-42.3801,  40.7463, -21.8935, -74.3954, -37.8143,  -8.6413],\n",
       "        [-27.2500,  88.8632,  -8.8330, 109.8314, -22.7258,  37.7100],\n",
       "        [ 49.0109,  16.9329, -37.6880,  20.9207,  39.5458, -37.6875],\n",
       "        [  1.6938,  -1.8043,  -1.4190,  13.6263,  14.5836, -19.7981]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt, ceil\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "# https://arxiv.org/pdf/2002.05202\n",
    "class MySwiGlu(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = int(ceil(8*d_model // 3 / 64) * 64)\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.silu = MySilu()\n",
    "        self.w1 = torch.randn((self.d_ff, self.d_model))\n",
    "        self.w2 = torch.randn((self.d_model, self.d_ff))\n",
    "        self.w3 = torch.randn((self.d_ff, self.d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # PyTorch way\n",
    "        # o1 = self.silu(x @ self.w1.T)\n",
    "        # o3 = x @ self.w3.T\n",
    "        # gated = o1 * o3\n",
    "        # o = gated @ self.w2.T\n",
    "\n",
    "        # einsum way\n",
    "        o1 = einsum(x, self.w1, \"... d_model, d_ff d_model -> ... d_ff\")\n",
    "        o3 = einsum(x, self.w3, \"... d_model, d_ff d_model -> ... d_ff\")\n",
    "        gated = self.silu(o1) * o3\n",
    "        o = einsum(gated, self.w2, \"... d_ff, d_model d_ff -> ... d_model\")\n",
    "        return o\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "model = MySwiGlu(6, 16)\n",
    "\n",
    "batch = torch.randn((4, 6))\n",
    "output = model(batch)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d0273",
   "metadata": {},
   "source": [
    "## Exercise 12: Problem (rope): Implement RoPE (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914e930",
   "metadata": {},
   "source": [
    "Rotation matrix derivation: https://www.youtube.com/watch?v=EZufiIwwqFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a63e3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 4, 8] n=64 x∈[-2.106, 2.110] μ=-0.066 σ=1.056\n",
       "tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431,\n",
       "          -1.6047],\n",
       "         [-1.7937,  0.2579, -0.2504, -1.4358, -0.7223, -0.5667, -0.7696,\n",
       "           0.7617],\n",
       "         [-0.5383,  1.5598, -0.5748,  0.3320, -0.7795,  1.0629,  0.7974,\n",
       "           1.6822],\n",
       "         [-1.4493, -1.1029,  0.1888,  1.4555, -0.2328,  0.0348, -0.2542,\n",
       "           0.8591]],\n",
       "\n",
       "        [[-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057,\n",
       "          -0.7746],\n",
       "         [-1.6794, -0.7727, -0.8154, -0.6860, -1.2953,  2.1099, -1.2342,\n",
       "          -0.4891],\n",
       "         [ 0.9787, -0.5571, -0.0280,  0.5308, -0.5117,  1.1814, -0.8125,\n",
       "          -0.7376],\n",
       "         [ 1.3841, -0.2337, -0.2603,  0.6267, -0.1531,  1.8408, -1.1887,\n",
       "           1.3800]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relearn this!\n",
    "class MyRoPE(torch.nn.Module):\n",
    "    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.theta = theta\n",
    "        self.d_k = d_k\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        self.rotation_matrix_table = self.generate_rotation_matrix(theta, d_k, max_seq_len)\n",
    "        self.register_buffer('rotation_matrix', self.rotation_matrix_table, persistent=False)\n",
    "\n",
    "    def generate_rotation_block(self, theta, block_index, seq_pos, d_k):\n",
    "        angle = torch.tensor(seq_pos / (theta ** (2 * block_index / d_k)))\n",
    "        cos = torch.cos(angle)\n",
    "        sin = torch.sin(angle)\n",
    "        rotation_matrix = torch.Tensor([[cos, -sin], [sin, cos]])\n",
    "        return rotation_matrix\n",
    "    \n",
    "    def generate_rotation_matrix(self, theta, d_k, max_seq_len):\n",
    "        rotation_matrix_table = torch.zeros(max_seq_len, d_k, d_k)\n",
    "        for i in range(max_seq_len):\n",
    "            blocks = [self.generate_rotation_block(theta, k, i, d_k) for k in range(d_k // 2)]\n",
    "            rotation_matrix_table[i, :, :] = torch.block_diag(*blocks)\n",
    "        return rotation_matrix_table\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, token_positions: torch.Tensor = None):\n",
    "        *dims, seq_len, d_k = x.shape\n",
    "        if token_positions is None:\n",
    "            token_positions = torch.arange(seq_len, device=x.device)\n",
    "        rotation_matrix = self.rotation_matrix_table[token_positions]\n",
    "        x_rotated = rotation_matrix @ x.unsqueeze(-1)\n",
    "        x_rotated = x_rotated.squeeze(-1)\n",
    "        return x_rotated\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "batch = torch.randn((2, 4, 8))\n",
    "model = MyRoPE(10000, 8, 12)\n",
    "tok_positions = torch.arange(4)\n",
    "output = model(batch, tok_positions)\n",
    "output.v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6a2fc",
   "metadata": {},
   "source": [
    "## Exercise 13: Problem (softmax): Implement softmax (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12f5579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[4, 8] n=32 x∈[0.007, 0.507] μ=0.125 σ=0.119\n",
       "tensor([[0.3971, 0.2558, 0.1423, 0.0070, 0.1139, 0.0168, 0.0554, 0.0116],\n",
       "        [0.0460, 0.5071, 0.0659, 0.0240, 0.0471, 0.0557, 0.0452, 0.2090],\n",
       "        [0.2693, 0.0444, 0.0317, 0.0809, 0.0244, 0.1532, 0.1161, 0.2799],\n",
       "        [0.2011, 0.2046, 0.1031, 0.2126, 0.0444, 0.0584, 0.0435, 0.1323]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MySoftmax(x: torch.Tensor, dim: int):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # numerical stability at high values of logits\n",
    "    # dim-wise max and not overall max\n",
    "    max = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    x = x - max\n",
    "\n",
    "    # usual business here\n",
    "    num = torch.exp(x)\n",
    "    denom = torch.exp(x).sum(dim=dim, keepdim=True)\n",
    "    output = num / (denom + eps)\n",
    "    return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch = torch.randn((4, 8))\n",
    "output = MySoftmax(batch, -1)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaeb2dd",
   "metadata": {},
   "source": [
    "## Exercise 14: Problem (scaled_dot_product_attention): Implement scaled dot-product attention (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a01b1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 4, 8] n=64 x∈[-1.408, 2.151] μ=0.076 σ=0.790\n",
       "tensor([[[-0.1561, -0.6953,  0.7203, -0.3861, -0.2114,  2.0479,  2.1509,\n",
       "           1.6704],\n",
       "         [-0.4206, -0.5915,  0.7682, -0.6597, -0.2825,  1.3089,  1.5757,\n",
       "           0.8848],\n",
       "         [-1.4079, -0.8897,  0.9736, -0.5634,  0.2719, -0.5261, -0.4475,\n",
       "           0.4843],\n",
       "         [-0.4120, -0.4365,  0.9151,  0.0516, -0.2064,  0.1070,  0.1036,\n",
       "           0.5917]],\n",
       "\n",
       "        [[ 1.9627, -0.3493, -0.4200, -0.1452, -0.2851, -1.0392,  0.1441,\n",
       "           0.8603],\n",
       "         [ 0.7856, -0.7519,  0.2780, -0.0635, -1.0282, -0.3954,  0.2636,\n",
       "           0.0057],\n",
       "         [-0.1239, -0.6923,  0.7523,  0.5994,  0.5321,  0.0085,  0.7380,\n",
       "          -1.0673],\n",
       "         [ 0.1558, -0.8902,  0.0420, -0.2327, -0.4623, -0.6036,  0.4148,\n",
       "          -0.4519]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def Myscaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    #  ## einsum approach\n",
    "    attn_scores = einsum(Q, K, \"... queries d_k, ... keys d_k -> ... queries keys\")\n",
    "    attn_weights = attn_scores / torch.sqrt(torch.tensor(Q.shape[-1]))\n",
    "\n",
    "    if mask is not None:\n",
    "        attn_weights = attn_weights.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "    attn_weights = MySoftmax(attn_weights, dim=-1)\n",
    "    context_vec = einsum(attn_weights, V, \"... queries sl, ... sl d_v -> ... queries d_v\")\n",
    "    return context_vec\n",
    "    \n",
    "torch.manual_seed(420)\n",
    "\n",
    "Q = torch.randn((2, 4, 8))\n",
    "K = torch.randn((2, 4, 8))\n",
    "V = torch.randn((2, 4, 8))\n",
    "\n",
    "# creating a mask\n",
    "mask = torch.randn((2, 4, 4))\n",
    "mask = torch.triu(mask, diagonal=1).to(bool)\n",
    "\n",
    "output = Myscaled_dot_product_attention(Q, K, V, ~mask)\n",
    "output.v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf30452c",
   "metadata": {},
   "source": [
    "## Exercise 15: Problem (multihead_self_attention): Implement causal multi-head self-attention (5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2441a193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6, 8] n=96 x∈[-37.604, 44.982] μ=1.520 σ=13.757 grad ViewBackward0\n",
       "tensor([[[ 44.9816,  -4.0572,   9.5668,  17.2590, -17.3515, -10.4152,  -7.6936,\n",
       "          -34.4534],\n",
       "         [ 14.4895,   2.1835,  26.0109,  20.7137, -21.6453,   4.8867,  -5.4112,\n",
       "           -2.5018],\n",
       "         [ -1.0056,  -4.4707, -16.1527,   2.4396,  11.6497,   0.6426,  11.3221,\n",
       "           -0.1446],\n",
       "         [  5.1057,  -6.8183, -17.2269,   4.7302,  14.0615,   1.3946,  11.8164,\n",
       "           -8.1424],\n",
       "         [-11.7404,  -3.6162, -10.8130, -17.4299,  11.6551,  -4.4994,  -0.2479,\n",
       "            3.2620],\n",
       "         [-24.8512,   7.8084,  22.2743,   3.1124, -10.6928,  30.2615,   5.8352,\n",
       "           30.7121]],\n",
       "\n",
       "        [[-19.2817,  -1.2965,   6.4948,  -8.5323,   2.0376,   7.6350,   2.2789,\n",
       "           13.9557],\n",
       "         [-37.6042,   6.6201,  10.2247,  -0.6709,   3.2950,  16.9392,   5.3303,\n",
       "           33.0000],\n",
       "         [ -2.9748,  -4.8768,   5.5871,  16.4281,   4.5379, -12.4719,  -1.8378,\n",
       "           -4.0125],\n",
       "         [-21.2404,   0.7599,  29.9460,   9.5963,  -9.8929,   5.2589,  -4.9882,\n",
       "           17.6042],\n",
       "         [ -1.5435,   5.8131,  -6.8222,  -5.0511,   1.0927,   4.3094,   2.0665,\n",
       "            2.2948],\n",
       "         [ -2.6097,   3.5150, -20.3922,  -6.9153,  11.8712,   4.8558,   6.2314,\n",
       "            2.5145]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "from torch.nn import Linear\n",
    "\n",
    "class MyCausalMHA(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_len=None, theta=10000, device=None, use_rope=False, token_positions=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_k = int(d_model / num_heads)\n",
    "        self.use_rope = use_rope\n",
    "        self.rope = MyRoPE(theta, self.d_k, max_seq_len) if use_rope else None\n",
    "        self.token_positions = token_positions\n",
    "\n",
    "        self.q_proj = Linear(d_model, d_model)\n",
    "        self.k_proj = Linear(d_model, d_model)\n",
    "        self.v_proj = Linear(d_model, d_model)\n",
    "        self.o_proj = Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, sl, d_model = x.shape\n",
    "\n",
    "        # queries = einsum(x, self.q_proj.weight, \"... sl d_m, d_m d_m -> ... sl d_m\")\n",
    "        # keys = einsum(x, self.k_proj.weight, \"... sl d_m, d_m d_m -> ... sl d_m\")\n",
    "        # values = einsum(x, self.v_proj.weight, \"... sl d_m, d_m d_m -> ... sl d_m\")\n",
    "\n",
    "        queries = self.q_proj(x)\n",
    "        keys = self.k_proj(x)\n",
    "        values = self.v_proj(x)\n",
    "\n",
    "        # Expand the head dimension into it's own dimension and transpose for self-attention soon\n",
    "        queries = rearrange(queries, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "        keys = rearrange(keys, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "        values = rearrange(values, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "\n",
    "        # use rope if needed\n",
    "        if self.use_rope:\n",
    "            queries = self.rope(queries, self.token_positions)\n",
    "            keys = self.rope(keys, self.token_positions)\n",
    "\n",
    "        # apply causal mask``\n",
    "        causal_mask = torch.ones((sl, sl))\n",
    "        causal_mask = torch.triu(causal_mask, diagonal=1).to(bool)\n",
    "        context_vec = Myscaled_dot_product_attention(queries, keys, values, ~causal_mask)\n",
    "\n",
    "        # concatenate head with o_projection & pass it through o_proj\n",
    "        context_vec = rearrange(context_vec, \"... h sl d_v -> ... sl (h d_v)\")\n",
    "        # output = einsum(context_vec, self.o_proj, \"... sl d_model, d_model d_v -> ... sl d_v\")\n",
    "        output = self.o_proj(context_vec)\n",
    "        return output\n",
    "\n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "m = torch.randn(8, 8)\n",
    "model = MyCausalMHA(d_model=8, num_heads=2, max_seq_len=10, use_rope=False)\n",
    "model.q_proj.weight.data.copy_(m)\n",
    "model.k_proj.weight.data.copy_(m)\n",
    "model.v_proj.weight.data.copy_(m)\n",
    "model.o_proj.weight.data.copy_(m)\n",
    "\n",
    "model.q_proj.bias.data.zero_()\n",
    "model.k_proj.bias.data.zero_()\n",
    "model.v_proj.bias.data.zero_()\n",
    "model.o_proj.bias.data.zero_()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch = torch.randn((2, 6, 8))\n",
    "context_vec1 = model(batch)\n",
    "context_vec1.v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc26356d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6, 8] n=96 x∈[-37.604, 44.982] μ=1.520 σ=13.757\n",
       "tensor([[[ 44.9816,  -4.0572,   9.5668,  17.2590, -17.3515, -10.4152,  -7.6936,\n",
       "          -34.4534],\n",
       "         [ 14.4895,   2.1835,  26.0109,  20.7137, -21.6453,   4.8867,  -5.4112,\n",
       "           -2.5018],\n",
       "         [ -1.0056,  -4.4707, -16.1527,   2.4396,  11.6497,   0.6426,  11.3221,\n",
       "           -0.1446],\n",
       "         [  5.1057,  -6.8183, -17.2269,   4.7302,  14.0615,   1.3946,  11.8164,\n",
       "           -8.1424],\n",
       "         [-11.7404,  -3.6162, -10.8130, -17.4299,  11.6551,  -4.4994,  -0.2479,\n",
       "            3.2620],\n",
       "         [-24.8512,   7.8084,  22.2743,   3.1124, -10.6928,  30.2615,   5.8352,\n",
       "           30.7121]],\n",
       "\n",
       "        [[-19.2817,  -1.2965,   6.4948,  -8.5323,   2.0376,   7.6350,   2.2789,\n",
       "           13.9557],\n",
       "         [-37.6042,   6.6201,  10.2247,  -0.6709,   3.2950,  16.9392,   5.3303,\n",
       "           33.0000],\n",
       "         [ -2.9748,  -4.8768,   5.5871,  16.4281,   4.5379, -12.4719,  -1.8378,\n",
       "           -4.0125],\n",
       "         [-21.2404,   0.7599,  29.9460,   9.5963,  -9.8929,   5.2589,  -4.9882,\n",
       "           17.6042],\n",
       "         [ -1.5435,   5.8131,  -6.8222,  -5.0511,   1.0927,   4.3094,   2.0665,\n",
       "            2.2948],\n",
       "         [ -2.6097,   3.5150, -20.3922,  -6.9153,  11.8712,   4.8558,   6.2314,\n",
       "            2.5145]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCausalMHA2(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, max_seq_len=None, theta=10000, device=None, use_rope=False, token_positions=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_k = int(d_model / num_heads)\n",
    "        self.use_rope = use_rope\n",
    "        self.rope = MyRoPE(theta, self.d_k, max_seq_len) if use_rope else None\n",
    "        self.token_positions = token_positions\n",
    "\n",
    "        self.q_proj = torch.randn(d_model, d_model)\n",
    "        self.k_proj = torch.randn(d_model, d_model)\n",
    "        self.v_proj = torch.randn(d_model, d_model)\n",
    "        self.o_proj = torch.randn(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, sl, d_model = x.shape\n",
    "\n",
    "        # einsum notation\n",
    "        queries = einsum(x, self.q_proj, \"... sl d_m, d_k d_m -> ... sl d_k\")\n",
    "        keys = einsum(x, self.k_proj, \"... sl d_m, d_k d_m -> ... sl d_k\")\n",
    "        values = einsum(x, self.v_proj, \"... sl d_m, d_k d_m -> ... sl d_k\")\n",
    "\n",
    "        # pytorch matrix notation\n",
    "        # CAREFUL: be very careful with transposing as PyTorch stores linear layers in (out_features, in_features) form\n",
    "        # queries = x @ self.q_proj.T\n",
    "        # keys = x @ self.k_proj.T\n",
    "        # values = x @ self.v_proj.T\n",
    "\n",
    "        # Expand the head dimension into it's own dimension and transpose for self-attention soon\n",
    "        queries = rearrange(queries, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "        keys = rearrange(keys, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "        values = rearrange(values, \"... sl (h d_head) -> ... h sl d_head\", h=self.num_heads)\n",
    "\n",
    "        # use rope if needed\n",
    "        if self.use_rope:\n",
    "            queries = self.rope(queries, self.token_positions)\n",
    "            keys = self.rope(keys, self.token_positions)\n",
    "\n",
    "        # apply causal mask\n",
    "        causal_mask = torch.ones((sl, sl))\n",
    "        causal_mask = torch.triu(causal_mask, diagonal=1).to(bool)\n",
    "        context_vec = Myscaled_dot_product_attention(queries, keys, values, ~causal_mask) # CAREFUL: causal_mask should not attend to future tokens\n",
    "\n",
    "        # concatenate head with o_projection & pass it through o_proj\n",
    "        context_vec = rearrange(context_vec, \"... h sl d_v -> ... sl (h d_v)\")\n",
    "        output = einsum(context_vec, self.o_proj, \"... sl d_model, d_v d_model -> ... sl d_v\")\n",
    "        # output = context_vec @ self.o_proj.T\n",
    "        return output\n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = MyCausalMHA2(d_model=8, num_heads=2, max_seq_len=10, use_rope=False)\n",
    "model.q_proj.data.copy_(m)\n",
    "model.k_proj.data.copy_(m)\n",
    "model.v_proj.data.copy_(m)\n",
    "model.o_proj.data.copy_(m)\n",
    "\n",
    "context_vec2 = model(batch)\n",
    "context_vec2.v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e21d85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_vec1, context_vec2, atol=1e-5, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a05c75",
   "metadata": {},
   "source": [
    "## Exercise 16: Problem (transformer_block): Implement the Transformer block (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d894ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 6, 8] n=96 x∈[-131.735, 125.800] μ=-0.974 σ=53.200 grad AddBackward0\n",
       "tensor([[[ -30.6578,   33.7612,   35.9949,  -13.0006, -113.8567,   44.8295,\n",
       "            94.9931,  -44.8292],\n",
       "         [ -34.0975,   51.0059,   41.1780,  -19.0532,  -78.3312,   31.9316,\n",
       "            57.9824,  -17.0674],\n",
       "         [ -72.8474, -131.7351,  -25.1189,  -12.0696,   11.0075,   16.2014,\n",
       "            44.7393,  -32.0036],\n",
       "         [ -38.0562,  -36.5616,   36.2781,    8.6351,  -61.7301,   39.3872,\n",
       "            76.6221,  -36.4891],\n",
       "         [ -18.7337,  -31.8101,   13.3985,  -25.0176,  -11.5342,   33.5430,\n",
       "            60.6820,  -36.9917],\n",
       "         [  29.1150,   28.7488,  -55.5202,  -41.5676,   31.3636,   41.8333,\n",
       "            34.3643,  -23.0237]],\n",
       "\n",
       "        [[  -5.0573,  125.7999,   20.2271,  -59.8220, -107.2766,   68.2812,\n",
       "            47.9998,  -14.3731],\n",
       "         [ -61.9032,  -57.3246,  -19.4308,    5.0884,   -2.7076,   21.9048,\n",
       "             2.9424,   33.5769],\n",
       "         [ -24.3774,  -10.0657,   42.8495,   60.4710,  -46.1791,  -40.8142,\n",
       "           -48.5564,  -13.8084],\n",
       "         [  70.8347,   27.2074,   68.1606,   94.7534,  -72.5252,  -16.0102,\n",
       "           -68.7625,   56.5744],\n",
       "         [ -25.6330,  -40.5323,   -2.3343,   44.2679,  -26.8085,   44.6750,\n",
       "            24.2750,  -19.5150],\n",
       "         [  14.8456,   50.5467,  -92.5855, -116.6030,  -13.9043,  101.9974,\n",
       "           121.9304,  -91.6963]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyTransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, max_seq_len=10000, theta=10000, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.device = device\n",
    "        self.attn = MyCausalMHA2(d_model=d_model, num_heads=num_heads, max_seq_len=max_seq_len, theta=theta, use_rope=True, device=device)\n",
    "        self.ffn = MySwiGlu(d_model=d_model, d_ff=d_ff, device=device, dtype=dtype)\n",
    "        self.attnorm = MyRMSNorm(d_model=d_model, device=device, dtype=dtype)\n",
    "        self.ffnnorm = MyRMSNorm(d_model=d_model, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        bs, sl, d_model = x.shape\n",
    "\n",
    "        x_norm = self.attnorm(x)\n",
    "        x_attn = self.attn(x_norm)\n",
    "        x_add = x + x_attn\n",
    "\n",
    "        x_ffn_norm = self.ffnnorm(x_add)\n",
    "        x_ffn = self.ffn(x_ffn_norm)\n",
    "        x_final = x_add + x_ffn\n",
    "        return x_final\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = MyTransformerBlock(d_model=8, num_heads=2, d_ff=64, max_seq_len=1000)\n",
    "\n",
    "batch = torch.randn((2, 6, 8))\n",
    "context_vec = model(batch)\n",
    "context_vec.v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f4fb6",
   "metadata": {},
   "source": [
    "## Exercise 17: Problem (transformer_lm): Implementing the Transformer LM (3 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9ddf79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 30, 32] n=1920 (7.5Kb) x∈[-2.094, 1.670] μ=-0.089 σ=0.636 grad ViewBackward0\n",
       "tensor([[[-0.2248, -1.1190,  0.3314,  ..., -0.2760, -0.6265,  1.1214],\n",
       "         [-0.5788, -0.2246, -0.9277,  ...,  1.1431,  0.4534,  0.0186],\n",
       "         [-0.7878, -0.4018, -1.0746,  ...,  1.3445,  0.0768,  0.1375],\n",
       "         ...,\n",
       "         [-0.4958, -0.4019, -0.8402,  ...,  0.7856, -0.2178,  0.4958],\n",
       "         [-0.0602,  0.7938, -0.6014,  ...,  1.2430, -0.3639,  0.0959],\n",
       "         [ 0.2141,  0.7943, -0.4072,  ...,  1.5604, -0.6140,  0.3407]],\n",
       "\n",
       "        [[ 0.4441, -0.1686, -0.6878,  ..., -1.2776,  0.2949, -0.2075],\n",
       "         [ 0.7333,  0.4126, -0.1541,  ..., -0.0763, -0.3868,  0.3773],\n",
       "         [ 0.6944,  0.4951,  0.2128,  ...,  0.5993, -0.6893,  0.6861],\n",
       "         ...,\n",
       "         [-0.3769,  0.5701, -0.2447,  ..., -0.0607,  0.9560, -1.2039],\n",
       "         [ 1.4656, -0.7442, -0.0486,  ..., -0.4798, -0.2522,  0.5383],\n",
       "         [ 1.2855,  0.7979,  0.4193,  ..., -0.9699, -0.7889,  0.2751]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyTransformerLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, context_length, d_model, num_layers, num_heads, d_ff, rope_theta=10000, device=None):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.rope_theta = rope_theta\n",
    "        self.device = device\n",
    "        self.tokembedding = MyEmbedding(num_embeddings=vocab_size, embedding_dim=d_model, device=device)\n",
    "        self.layers = [MyTransformerBlock(d_model=d_model, num_heads=num_heads, d_ff=d_ff, max_seq_len=context_length, theta=rope_theta) for _ in range(self.num_layers)]\n",
    "        self.norm = MyRMSNorm(d_model=d_model, eps=1e-5, device=device)\n",
    "        self.linear = MyLinear(d_model, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokembedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) # CAREFUL: these are layers and not heads\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        # x = MySoftmax(x, -1) # commenting as the unit test expects us to return unnormalized logits\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = MyTransformerLM(vocab_size=32, context_length = 1024, d_model=8, num_layers=3, num_heads=2, d_ff=64, rope_theta=10000)\n",
    "\n",
    "batch = torch.randn((2, 6, 8))\n",
    "batch = torch.randint(0, 32, (2, 30, ))\n",
    "context_vec = model(batch)\n",
    "context_vec.v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2720690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d6069a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.W.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c057509",
   "metadata": {},
   "source": [
    "Number of parameters\n",
    "\n",
    "\n",
    "* 1. tokembedding\n",
    "\n",
    "\n",
    "vocab_size * d_model = 32 * 8 = 256\n",
    "\n",
    "code:\n",
    "`model.tokembedding.W.numel()`\n",
    "\n",
    "---\n",
    "\n",
    "* 2. attns:\n",
    "attn -> num_layers * (q_proj + k_proj + v_proj + o_proj) = num_layers * (d_model ** 2 * 4) = 3 * 8**2 * 4 = 768\n",
    "\n",
    "code:\n",
    "`model.attns[0].attn.q_proj.numel() * 4 * num_layers`\n",
    "\n",
    "ffn -> num_layers * (w1 + w2 + w3) = num_layers * (d_ff * d_model * 3) = 3 * 64 * 8 * 3 = 4608\n",
    "\n",
    "code:\n",
    "`model.attns[0].ffn.w1.numel() * 3 * num_layers`\n",
    "\n",
    "attnorm -> num_layers * d_model = 3 * 8 = 24\n",
    "\n",
    "code:\n",
    "`model.attns[0].attnorm.W.numel()`\n",
    "\n",
    "ffnnorm -> num_layers * d_model = 3 * 8 = 24\n",
    "\n",
    "code:\n",
    "`model.attns[0].ffnnorm.W.numel()`\n",
    "\n",
    "---\n",
    "\n",
    "3. norm -> d_model = 8\n",
    "\n",
    "code:\n",
    "`model.norm.W.numel()`\n",
    "\n",
    "4. linear -> d_model * vocab_size = 8 * 32 = 2048\n",
    "\n",
    "code:\n",
    "`model.linear.W.numel()`\n",
    "\n",
    "Total = 256 + 768 + 4608 + 24 + 24 + 8 + 2048 = 48736 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea677c3",
   "metadata": {},
   "source": [
    "## Exercise 18: Problem (transformer_accounting): Transformer LM resource accounting (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e386205",
   "metadata": {},
   "source": [
    "(a) Consider GPT-2 XL, which has the following configuration:\n",
    "```\n",
    "vocab_size : 50,257\n",
    "context_length : 1,024\n",
    "num_layers : 48\n",
    "d_model : 1,600\n",
    "num_heads : 25\n",
    "d_ff : 6,400\n",
    "```\n",
    "Suppose we constructed our model using this configuration. How many trainable parameters\n",
    "would our model have? Assuming each parameter is represented using single-precision floating\n",
    "point, how much memory is required to just load this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b28163c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1637945600"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_param_count(vocab_size, context_length, num_layers, d_model, num_heads, d_ff):\n",
    "    # embedding\n",
    "    emb_params = vocab_size * d_model\n",
    "    emb_params += context_length * d_model # position embedding\n",
    "\n",
    "    # attention\n",
    "    attn_params = num_layers * (d_model ** 2 * 4)\n",
    "\n",
    "    # norm\n",
    "    attnorm_params = num_layers * d_model * 2 # for bias\n",
    "    ffnnorm_params =  num_layers * d_model * 2 # for bias\n",
    "    finalnorm_params = d_model * 2 # for weight and bias\n",
    "    norm_params = attnorm_params + ffnnorm_params + finalnorm_params\n",
    "\n",
    "    # MLP/linear\n",
    "    ffn_params =  num_layers * (d_ff * d_model * 2 + d_ff * 2) # for gpt2xl\n",
    "\n",
    "    # output projection\n",
    "    output_params = d_model * vocab_size\n",
    "\n",
    "    params = {\"emb_params\": emb_params,\n",
    "                    \"attn_params\": attn_params,\n",
    "                    \"ffn_params\": ffn_params,\n",
    "                    \"norm_params\": norm_params,\n",
    "                    \"output_params\": output_params}\n",
    "    return params\n",
    "\n",
    "params = get_param_count(vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400)\n",
    "sum(params.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828c709",
   "metadata": {},
   "source": [
    "The total number of parameters in GPT2-XL are 1.64B parameters.\n",
    "\n",
    "Here is the split across layers.\n",
    "\n",
    "```\n",
    "{'emb_params': 82049600,\n",
    " 'attn_params': 491520000,\n",
    " 'ffn_params': 983654400,\n",
    " 'norm_params': 310400,\n",
    " 'output_params': 80411200}\n",
    "```\n",
    "\n",
    "CAREFUL: Note that GPT2-XL does not use SwiGLU feed forward layers. Hence, the MLP/linear layer param count is much lesser than the architecture we used earlier.\n",
    "\n",
    "In terms of memory, as each parameter is stored as a single precision floating point, i.e. float32 or FP32, the memory needed to hold these weights in memory will be ~1.64B * 4 bytes. = 6.56GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062d905",
   "metadata": {},
   "source": [
    "b) Identify the matrix multiplies required to complete a forward pass of our GPT-2 XL-shaped\n",
    "model. How many FLOPs do these matrix multiplies require in total? Assume that our input\n",
    "sequence has context_length tokens.\n",
    "Deliverable: A list of matrix multiplies (with descriptions), and the total number of FLOPs\n",
    "required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee55851",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce609837",
   "metadata": {},
   "source": [
    "1. Embedding module just indexes and fetches the embedding vector for each token. Hence, no matrix multiplies.\n",
    "2. For RMSNorm module, here are the flop calculations.:\n",
    "* For each sequence position (1bs and 1sl), we have a hidden_dim vector for which the square operation is `hidden_dim` flops.\n",
    "* Sum is `hidden_dim-1` flops.\n",
    "* Division for mean calculation is 1 flop\n",
    "* sqrt is 1 flop\n",
    "* eps addition is 1 flop\n",
    "* Division by RMS is `hidden_dim` flops\n",
    "* Total flops for one sequence position is 3 * hidden_dim + 2 == `3 * hidden_dim`\n",
    "* Total flops for RMSNorm are `3 * hidden_dim * seq_len * batch_size`.\n",
    "3. For each transformer block,\n",
    "* RMSNorm is `3 * batch_size * seq_len * hidden_dim` computations\n",
    "* FFNNorm is the same as above.\n",
    "* 2 residual connections sum upto `2 * batch_size * seq_len * hidden_dim`\n",
    "* What is remaining is the causal attention block and position wise FFN - `(4 * seq_len**2 * hidden_dim + 4 * seq_len * d_model * hidden_dim) * n_heads`\n",
    "* So, for all transformer blocks, number of flops are `num_layers * ()`\n",
    "4. For the linear layer, the flop count is `2 * batch_size * seq_len * hidden_dim * vocab_size`\n",
    "\n",
    "# Codifying this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d48fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emb_flops': 0, 'attn_flops': 1332530380800, 'ffn_flops': 2013265920000, 'norm_flops': 476774400, 'output_flops': 164682137600, 'residual_flops': 157286400}\n",
      "Total flops: 3511B flops\n"
     ]
    }
   ],
   "source": [
    "def get_flop_count(vocab_size, context_length, num_layers, d_model, num_heads, d_ff, batch_size):\n",
    "    # embedding\n",
    "    emb_flops = 0\n",
    "\n",
    "    # attention\n",
    "    d_k = d_model // num_heads\n",
    "    qkv_proj_flops = 3 * 2 * batch_size * context_length * d_model * d_model # 2mnp formula\n",
    "    qk_flops = num_heads * 2 * batch_size * context_length * context_length * d_k\n",
    "    softmax_flops = num_heads * 3 * batch_size * context_length * context_length # approx 3 ops per element * number of heads\n",
    "    attn_v_flops = num_heads * 2 * batch_size * context_length * context_length * d_k\n",
    "    o_proj_flops = 2 * batch_size * context_length * d_model * d_model\n",
    "\n",
    "    attn_flops_each_layer = qkv_proj_flops + qk_flops + softmax_flops + attn_v_flops + o_proj_flops\n",
    "    attn_flops = num_layers * attn_flops_each_layer\n",
    "\n",
    "    # rms norm (3 per layer, attn_norm, ffn_norm, final_norm)\n",
    "    norm_flops_per_layer = 3 * batch_size * context_length * d_model # CAREFUL: calculation for each layer\n",
    "    norm_flops = (2 * num_layers + 1) * norm_flops_per_layer\n",
    "\n",
    "    # MLP/linear\n",
    "    # CAREFUL: GPT-2 uses standard FFN and not SwiGLU. So, just 2 matrix multiplies\n",
    "    ffn_flops = 2 * batch_size * context_length * d_model * d_ff + \\\n",
    "                2 * batch_size * context_length * d_ff * d_model\n",
    "    \n",
    "    ffn_flops = num_layers * ffn_flops\n",
    "    # ffn_params =  num_layers * (d_ff * d_model * 2 + d_ff * 2) # for gpt2xl\n",
    "\n",
    "    # output projection\n",
    "    output_flops = 2 * batch_size * context_length * d_model * vocab_size\n",
    "    # output_params = d_model * vocab_size\n",
    "\n",
    "    # residual connections\n",
    "    residual_flops = 2 * batch_size * context_length * d_model * num_layers # CAREFUL: 2 residuals per layer\n",
    "\n",
    "    flops = {\"emb_flops\": emb_flops,\n",
    "            \"attn_flops\": attn_flops,\n",
    "            \"ffn_flops\": ffn_flops,\n",
    "            \"norm_flops\": norm_flops,\n",
    "            \"output_flops\": output_flops,\n",
    "            \"residual_flops\": residual_flops}\n",
    "    return flops\n",
    "\n",
    "flops = get_flop_count(vocab_size=50257, context_length=1024, num_layers=48, d_model=1600, num_heads=25, d_ff=6400, batch_size=1)\n",
    "print(flops)\n",
    "print(f\"Total flops: {round(sum(flops.values()) / 1e9)}B flops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "686f6038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "emb_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "attn_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ffn_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "norm_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "output_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "residual_flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total Flops",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Attention FLOP %",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FFN FLOP %",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b526a650-0253-4fba-98c8-2c7a6b200645",
       "rows": [
        [
         "0",
         "gpt2_small",
         "0",
         "97089748992",
         "241591910400",
         "58982400",
         "79047426048",
         "18874368",
         "417806942208",
         "23.2",
         "76.7"
        ],
        [
         "1",
         "gpt2_medium",
         "0",
         "310445604864",
         "644245094400",
         "154140672",
         "105396568064",
         "50331648",
         "1060291739648",
         "29.3",
         "70.7"
        ],
        [
         "2",
         "gpt2_large",
         "0",
         "678722273280",
         "1207959552000",
         "287047680",
         "131745710080",
         "94371840",
         "2018808954880",
         "33.6",
         "66.4"
        ],
        [
         "3",
         "gpt2_xl_sl1k",
         "0",
         "1332530380800",
         "2013265920000",
         "476774400",
         "164682137600",
         "157286400",
         "3511112499200",
         "38.0",
         "62.0"
        ],
        [
         "4",
         "gpt2_xl_sl16k",
         "0",
         "99535867084800",
         "32212254720000",
         "7628390400",
         "2634914201600",
         "2516582400",
         "134393180979200",
         "74.1",
         "25.9"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>emb_flops</th>\n",
       "      <th>attn_flops</th>\n",
       "      <th>ffn_flops</th>\n",
       "      <th>norm_flops</th>\n",
       "      <th>output_flops</th>\n",
       "      <th>residual_flops</th>\n",
       "      <th>Total Flops</th>\n",
       "      <th>Attention FLOP %</th>\n",
       "      <th>FFN FLOP %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2_small</td>\n",
       "      <td>0</td>\n",
       "      <td>97089748992</td>\n",
       "      <td>241591910400</td>\n",
       "      <td>58982400</td>\n",
       "      <td>79047426048</td>\n",
       "      <td>18874368</td>\n",
       "      <td>417806942208</td>\n",
       "      <td>23.2</td>\n",
       "      <td>76.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2_medium</td>\n",
       "      <td>0</td>\n",
       "      <td>310445604864</td>\n",
       "      <td>644245094400</td>\n",
       "      <td>154140672</td>\n",
       "      <td>105396568064</td>\n",
       "      <td>50331648</td>\n",
       "      <td>1060291739648</td>\n",
       "      <td>29.3</td>\n",
       "      <td>70.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2_large</td>\n",
       "      <td>0</td>\n",
       "      <td>678722273280</td>\n",
       "      <td>1207959552000</td>\n",
       "      <td>287047680</td>\n",
       "      <td>131745710080</td>\n",
       "      <td>94371840</td>\n",
       "      <td>2018808954880</td>\n",
       "      <td>33.6</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2_xl_sl1k</td>\n",
       "      <td>0</td>\n",
       "      <td>1332530380800</td>\n",
       "      <td>2013265920000</td>\n",
       "      <td>476774400</td>\n",
       "      <td>164682137600</td>\n",
       "      <td>157286400</td>\n",
       "      <td>3511112499200</td>\n",
       "      <td>38.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt2_xl_sl16k</td>\n",
       "      <td>0</td>\n",
       "      <td>99535867084800</td>\n",
       "      <td>32212254720000</td>\n",
       "      <td>7628390400</td>\n",
       "      <td>2634914201600</td>\n",
       "      <td>2516582400</td>\n",
       "      <td>134393180979200</td>\n",
       "      <td>74.1</td>\n",
       "      <td>25.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model  emb_flops      attn_flops       ffn_flops  norm_flops  \\\n",
       "0     gpt2_small          0     97089748992    241591910400    58982400   \n",
       "1    gpt2_medium          0    310445604864    644245094400   154140672   \n",
       "2     gpt2_large          0    678722273280   1207959552000   287047680   \n",
       "3   gpt2_xl_sl1k          0   1332530380800   2013265920000   476774400   \n",
       "4  gpt2_xl_sl16k          0  99535867084800  32212254720000  7628390400   \n",
       "\n",
       "    output_flops  residual_flops      Total Flops  Attention FLOP %  \\\n",
       "0    79047426048        18874368     417806942208              23.2   \n",
       "1   105396568064        50331648    1060291739648              29.3   \n",
       "2   131745710080        94371840    2018808954880              33.6   \n",
       "3   164682137600       157286400    3511112499200              38.0   \n",
       "4  2634914201600      2516582400  134393180979200              74.1   \n",
       "\n",
       "   FFN FLOP %  \n",
       "0        76.7  \n",
       "1        70.7  \n",
       "2        66.4  \n",
       "3        62.0  \n",
       "4        25.9  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpt2_configs = [\n",
    "    {\n",
    "        \"name\": \"gpt2_small\",\n",
    "        \"num_layers\": 12,\n",
    "        \"d_model\": 768,\n",
    "        \"num_heads\": 12,\n",
    "        \"batch_size\": 1,\n",
    "        \"context_length\": 1024,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"d_ff\": 6400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt2_medium\",\n",
    "        \"num_layers\": 24,\n",
    "        \"d_model\": 1024,\n",
    "        \"num_heads\": 16,\n",
    "        \"batch_size\": 1,\n",
    "        \"context_length\": 1024,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"d_ff\": 6400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt2_large\",\n",
    "        \"num_layers\": 36,\n",
    "        \"d_model\": 1280,\n",
    "        \"num_heads\": 20,\n",
    "        \"batch_size\": 1,\n",
    "        \"context_length\": 1024,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"d_ff\": 6400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt2_xl_sl1k\",\n",
    "        \"num_layers\": 48,\n",
    "        \"d_model\": 1600,\n",
    "        \"num_heads\": 25,\n",
    "        \"batch_size\": 1,\n",
    "        \"context_length\": 1024,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"d_ff\": 6400\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gpt2_xl_sl16k\",\n",
    "        \"num_layers\": 48,\n",
    "        \"d_model\": 1600,\n",
    "        \"num_heads\": 25,\n",
    "        \"batch_size\": 1,\n",
    "        \"context_length\": 16384,\n",
    "        \"vocab_size\": 50257,\n",
    "        \"d_ff\": 6400\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in gpt2_configs:\n",
    "    flops = {}\n",
    "    flops['model'] = config['name']\n",
    "    config_args = {k: v for k, v in config.items() if k != 'name'}\n",
    "    flop_counts = get_flop_count(**config_args)\n",
    "    flops.update(flop_counts)\n",
    "    flops['Total Flops'] = int(sum(flop_counts.values()))\n",
    "    flops['Attention FLOP %'] = round(flops['attn_flops'] * 100 / flops['Total Flops'], 1)\n",
    "    flops['FFN FLOP %'] = round((flops['ffn_flops'] + flops['output_flops']) * 100 / flops['Total Flops'], 1)\n",
    "    results.append(flops)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c881dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL\n",
    "# Rearrange operations are flop free! They do not consume flops.\n",
    "# No mathematical operations are happening at this time. Memory layout is changing.\n",
    "# Some of the other operations that are FLOP-free are view, stack, cat, squeeze, unsqueeze, transpose, permute, reshape etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a0119cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Based on your analysis above, which parts of the model require the most FLOPs\n",
    "# Answer: FFN layers require the most FLOPS at all model scales. This is where a lot of compute time is being spent.\n",
    "\n",
    "# (d) Repeat your analysis with GPT-2 small (12 layers, 768 d_model, 12 heads), GPT-2 medium (24\n",
    "# layers, 1024 d_model, 16 heads), and GPT-2 large (36 layers, 1280 d_model, 20 heads). As the\n",
    "# model size increases, which parts of the Transformer LM take up proportionally more or less of\n",
    "# the total FLOPs?\n",
    "# Deliverable: For each model, provide a breakdown of model components and its associated\n",
    "# FLOPs (as a proportion of the total FLOPs required for a forward pass). In addition, provide a\n",
    "# one-to-two sentence description of how varying the model size changes the proportional FLOPs\n",
    "# of each component.\n",
    "\n",
    "# Answer: The above table provides the breakdown of FLOP% across attention and FFN layers as the model size increases. A few insights:\n",
    "# 1. At low model sizes, the attention flops is around 30% of total FLOPS.\n",
    "# 2. As context length grows, attention flops % significantly goes up and slowing down the inference.\n",
    "\n",
    "# (e) Take GPT-2 XL and increase the context length to 16,384. How does the total FLOPs for one\n",
    "# forward pass change? How do the relative contribution of FLOPs of the model components\n",
    "# change\n",
    "# As GPT-2 XL model observes a context length increase from 1k to 16k, the attention flops go up \n",
    "# from occupying 40% of total flops to 75% of total flops!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0468d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ptflops import get_model_complexity_info\n",
    "\n",
    "# layer = MyRMSNorm(d_model=8)\n",
    "# torch.manual_seed(42)\n",
    "# batch = torch.randn(2, 4, 8)\n",
    "\n",
    "# macs, params = get_model_complexity_info(layer, tuple(batch.shape), print_per_layer_stat=True, verbose=True)\n",
    "# print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "# print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889dafe",
   "metadata": {},
   "source": [
    "## Exercise 19: Problem (cross_entropy): Implement Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e0d4d3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8959376811981201"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def MyCrossEntropy(logits, targs):\n",
    "    eps = 1e-8\n",
    "    probs = MySoftmax(logits, -1)\n",
    "    targs_onehot = F.one_hot(targs)\n",
    "    loss = - (targs_onehot * (probs + eps).log()).sum(-1)\n",
    "    return loss.mean()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# logits = torch.randn(4, 3)\n",
    "# targs = torch.randint(0, 3, (4,))\n",
    "\n",
    "logits = torch.randn(2, 4, 3)\n",
    "targs = torch.randint(0, 3, (2, 4,))\n",
    "\n",
    "loss = MyCrossEntropy(logits, targs)\n",
    "loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a84ae725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor 1.896"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn(rearrange(logits, \"b s d -> (b s) d\"), rearrange(targs, \"b s -> (b s)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6d3d52de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8959378004074097 1.8959379196166992\n"
     ]
    }
   ],
   "source": [
    "def MySoftmax(x: torch.Tensor, dim: int):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # numerical stability at high values of logits\n",
    "    # dim-wise max and not overall max\n",
    "    max = torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    x = x - max\n",
    "\n",
    "    # usual business here\n",
    "    num = torch.exp(x)\n",
    "    denom = torch.exp(x).sum(dim=dim, keepdim=True)\n",
    "    output = num / (denom + eps)\n",
    "    return output\n",
    "\n",
    "def MyLogSoftmax(x, dim):\n",
    "    # log_softmax is equal to log(softmax values)\n",
    "    # log(softmax values) = log(num / denom) = log(num) - log(denom)\n",
    "    # log(num) = log(torch.exp(x)) = x # cancel log and exp\n",
    "    # log(denom) = log(torch.exp(x).sum(dim, keepdim=True))\n",
    "    # log_softmax = x - log(torch.exp(x).sum(dim, keepdim=True))\n",
    "\n",
    "    eps = 1e-8\n",
    "    max = torch.max(x, dim, keepdim=True)[0]\n",
    "    x_shifted = x - max\n",
    "\n",
    "    output = x_shifted - torch.log(torch.exp(x_shifted).sum(dim=dim, keepdim=True))\n",
    "    return output\n",
    "\n",
    "\n",
    "def MyCrossEntropy(logits, targs):\n",
    "    eps = 1e-8\n",
    "    # probs = MySoftmax(logits, -1) # torch.exp might create issues here. Instead, consider log_softmax.\n",
    "    # probs = torch.log_softmax(logits, -1)\n",
    "    probs = MyLogSoftmax(logits, -1)\n",
    "    num_classes = logits.shape[-1]\n",
    "    targs_onehot = F.one_hot(targs, num_classes=num_classes) # CAREFUL: Assign num_classes. Otherwise, absence of a class in target might not yield enough classes.\n",
    "    loss = - (targs_onehot * (probs)).sum(-1)\n",
    "    return loss.mean()\n",
    "\n",
    "loss1 = MyCrossEntropy(logits, targs)\n",
    "loss1.item()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss2 = loss_fn(rearrange(logits, \"b s d -> (b s) d\"), rearrange(targs, \"b d -> (b d)\"))\n",
    "\n",
    "print(loss1.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "70cd6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def printt(*args):\n",
    "    frame = inspect.currentframe().f_back\n",
    "    names = {id(v): k for k, v in frame.f_locals.items()}\n",
    "    for arg in args:\n",
    "        var_name = names.get(id(arg), None)\n",
    "        print(f\"{var_name if var_name else '<unknown>'}:\")\n",
    "        print(arg.v)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "077f9b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor[4, 3] n=12 x∈[-1.123, 2.208] μ=0.272 σ=0.806\n",
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617],\n",
      "        [ 0.2674,  0.5349,  0.8094]])\n",
      "\n",
      "\n",
      "targs:\n",
      "tensor[4] i64 x∈[0, 2] μ=1.250 σ=0.957 [2, 1, 2, 0]\n",
      "tensor([2, 1, 2, 0])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "logits = torch.randn(4, 3)\n",
    "targs = torch.randint(0, 3, (4,))\n",
    "\n",
    "printt(logits, targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fd0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
