{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7bca9b",
   "metadata": {},
   "source": [
    "# CS336 Assignments\n",
    "\n",
    "| # | Topic                         | Description                                 |\n",
    "|---|-------------------------------|---------------------------------------------|\n",
    "| 1 | Basics                        | Train an LLM from scratch                   |\n",
    "| 2 | Systems                       | Make it run fast!                           |\n",
    "| 3 | Scaling                       | Make it performant at a FLOP budget         |\n",
    "| 4 | Data                          | Prepare the right datasets                  |\n",
    "| 5 | Alignment & Reasoning RL      | Align it to real-world use cases            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f19a5c",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "- Implement all of the components (tokenizer, model, loss function, optimizer) necessary to train a standard Transformer language model\n",
    "- Train a minimal language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fad44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c035e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropyloss(actuals, preds):\n",
    "    probs = preds.squeeze().sigmoid()\n",
    "    eps = 1e-8\n",
    "    loss = - actuals * (probs + eps).log() - (1 - actuals) * ((1 - probs) + eps).log()\n",
    "    return loss.mean()\n",
    "\n",
    "class AdamW_custom:\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-2):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.wd = weight_decay\n",
    "        self.eps = eps\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Initialize momentum and velocity for each parameter\n",
    "        self.m = [torch.zeros_like(p) for p in self.params]\n",
    "        self.v = [torch.zeros_like(p) for p in self.params]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad = param.grad.data\n",
    "\n",
    "            # Apply weight decay directly to parameters (AdamW style)\n",
    "            param.data.mul_(1 - self.lr * self.wd)\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            # beta1 * i + (1-beta1) * grad\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad.pow(2)\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.step_count)\n",
    "\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.step_count)\n",
    "\n",
    "            # Update parameters\n",
    "            param.data.add_(m_hat * -self.lr/ (v_hat.sqrt() + self.eps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b879e",
   "metadata": {},
   "source": [
    "## layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e74ad8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[0., 1.286] μ=0.331 σ=0.392 grad ReluBackward0\n",
       "tensor([[0.2151, 0.1047, 0.0000, 0.0000, 0.2167, 0.0521],\n",
       "        [0.4768, 0.0000, 0.7975, 0.3168, 1.2863, 0.6434],\n",
       "        [0.0000, 0.0000, 0.9230, 0.7498, 0.1746, 0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFN()\n",
    "output = model(data)\n",
    "\n",
    "output.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb9b941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[3] x∈[0.098, 0.587] μ=0.331 σ=0.245 grad MeanBackward1 [0.098, 0.587, 0.308],\n",
       " tensor[3] x∈[0.010, 0.193] μ=0.126 σ=0.101 grad VarBackward0 [0.010, 0.193, 0.175])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = output.mean(dim=-1)\n",
    "var = output.var(dim=-1)\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292aa088",
   "metadata": {},
   "source": [
    "We see that the mean of each layer is not zero and the standard deviation is not close to 1. With layer normalization, we can bring them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09da29ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m (\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m) / torch.sqrt(var)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13f7503a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor[3, 1] x∈[0.098, 0.587] μ=0.331 σ=0.245 grad MeanBackward1 [[0.098], [0.587], [0.308]],\n",
       " tensor[3, 1] x∈[0.010, 0.193] μ=0.126 σ=0.101 grad VarBackward0 [[0.010], [0.193], [0.175]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "759eb4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[-1.335, 1.591] μ=-6.623e-09 σ=0.939 grad DivBackward0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c87eb5c",
   "metadata": {},
   "source": [
    "0 mean and 1 variance! :) \n",
    "\n",
    "Let's wrap this as a PyTorch layer now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa3ce696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm_custom(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        # self.scale = torch.nn.Parameter(torch.randn())\n",
    "        # self.shift = torch.nn.Parameter(torch.randn())\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "\n",
    "        norm_tensor = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709161b7",
   "metadata": {},
   "source": [
    "Let's add this to the above FFN class and see if the outputs look normalized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23c7a461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] n=15 x∈[-1.776, 1.646] μ=3.974e-09 σ=0.926"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ln = LayerNorm_custom()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.relu(self.linear(x))\n",
    "        o = self.ln(x)\n",
    "        return o\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "547a5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm_custom2(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = torch.nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = torch.nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True)\n",
    "\n",
    "        norm_tensor = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        output = self.scale * norm_tensor + self.shift\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b0c00fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] n=15 x∈[-1.776, 1.646] μ=3.974e-09 σ=0.926 grad DivBackward0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.ln = LayerNorm_custom2(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.relu(self.linear(x))\n",
    "        o = self.ln(x)\n",
    "        return o\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "\n",
    "\n",
    "mean = output.mean(dim=-1, keepdim=True)\n",
    "var = output.var(dim=-1, keepdim=True)\n",
    "\n",
    "(output - mean) / torch.sqrt(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5705a6",
   "metadata": {},
   "source": [
    "## GeLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "073095ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 6] n=18 x∈[-1.151, 1.720] μ=1.325e-08 σ=0.939 grad DivBackward0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GeLU_custom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2) / torch.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        return o\n",
    "    \n",
    "\n",
    "class FFNWithNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 6)\n",
    "        self.act = GeLU_custom()\n",
    "        self.ln = LayerNorm_custom()\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.linear(x)\n",
    "        o = self.act(o)\n",
    "        o = self.ln(o)\n",
    "        return o\n",
    "\n",
    "torch.manual_seed(42)\n",
    "data = torch.randn(3, 5)\n",
    "model = FFNWithNorm()\n",
    "output = model(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f0277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
